{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrea212023/ML-Kis/blob/main/HW_2_4_Boosting_algorithms_KIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В цьому домашньому завданні ми знову працюємо з даними з нашого змагання [\"Bank Customer Churn Prediction (DLU Course)\"](https://www.kaggle.com/t/7c080c5d8ec64364a93cf4e8f880b6a0).\n",
        "\n",
        "Тут ми побудуємо рішення задачі класифікації з використанням алгоритмів бустингу: XGBoost та LightGBM, а також використаємо бібліотеку HyperOpt для оптимізації гіперпараметрів."
      ],
      "metadata": {
        "id": "fDefDHQt8LXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Зчитайте дані `train.csv` в змінну `raw_df` та скористайтесь наведеним кодом нижче аби розділити дані на трнувальні та валідаційні і розділити дані на ознаки з матириці Х та цільову змінну. Назви змінних `train_inputs, train_targets, train_inputs, train_targets` можна змінити на ті, які Вам зручно.\n",
        "\n",
        "  Наведений скрипт - частина отриманого мною скрипта для обробки даних. Ми тут не викнуємо масштабування та обробку категоріальних змінних, бо хочемо це делегувати алгоритмам, які будемо використовувати. Якщо щось не розумієте в наведених скриптах, рекомендую розібратись: навичка читати код - важлива складова роботи в машинному навчанні."
      ],
      "metadata": {
        "id": "LhivzW9W8-Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x68B5xbvpCCi",
        "outputId": "74da0a06-43d6-4dd9-d620-13c439885e8c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --upgrade --quiet\n",
        "!pip install hyperopt --quiet"
      ],
      "metadata": {
        "id": "tzBvarl2o9pC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
      ],
      "metadata": {
        "id": "s_UpQhsto00x"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "from typing import Tuple, Dict, Any"
      ],
      "metadata": {
        "id": "tg6jsDpkpq6B"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = 'https://www.kaggle.com/competitions/bank-customer-churn-prediction-dlu/'\n",
        "od.download(dataset_url)\n",
        "data_dir = './bank-customer-churn-prediction-dlu'\n",
        "raw_df = pd.read_csv('./bank-customer-churn-prediction-dlu/train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5eR-kaJpqjN",
        "outputId": "8a5bb404-d939-4c1f-8052-62b6bc2d830b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./bank-customer-churn-prediction-dlu\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "\n",
        "def split_train_val(df: pd.DataFrame, target_col: str, test_size: float = 0.2, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split the dataframe into training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The raw dataframe.\n",
        "        target_col (str): The target column for stratification.\n",
        "        test_size (float): The proportion of the dataset to include in the validation split.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: Training and validation dataframes.\n",
        "    \"\"\"\n",
        "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_col])\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def separate_inputs_targets(df: pd.DataFrame, input_cols: list, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Separate inputs and targets from the dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe.\n",
        "        input_cols (list): List of input columns.\n",
        "        target_col (str): Target column.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.Series]: DataFrame of inputs and Series of targets.\n",
        "    \"\"\"\n",
        "    inputs = df[input_cols].copy()\n",
        "    targets = df[target_col].copy()\n",
        "    return inputs, targets"
      ],
      "metadata": {
        "id": "cKE8RTPf6CRD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_column = 'Exited'\n",
        "input_columns = raw_df.columns.difference([target_column]).tolist()\n",
        "\n",
        "train_df, val_df = split_train_val(raw_df, target_col=target_column)\n",
        "train_inputs, train_targets = separate_inputs_targets(train_df, input_cols=input_columns, target_col=target_column)\n",
        "val_inputs, val_targets = separate_inputs_targets(val_df, input_cols=input_columns, target_col=target_column)\n",
        "\n",
        "# Print results\n",
        "print(\"Train inputs:\")\n",
        "print(train_inputs.head())\n",
        "print(\"Train targets:\")\n",
        "print(train_targets.head())\n",
        "print(\"Validation inputs:\")\n",
        "print(val_inputs.head())\n",
        "print(\"Validation targets:\")\n",
        "print(val_targets.head())"
      ],
      "metadata": {
        "id": "-bHdMJVB4xQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c17236-5d76-4625-abf2-c62d9990267e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train inputs:\n",
            "        Age    Balance  CreditScore  CustomerId  EstimatedSalary  Gender  \\\n",
            "7180   30.0  131394.56        682.0  15652218.0        143952.24    Male   \n",
            "10393  39.0  178058.06        684.0  15592937.0        145518.31  Female   \n",
            "80     35.0  116320.68        705.0  15774586.0        174431.01    Male   \n",
            "3365   58.0       0.00        669.0  15780572.0         51565.98    Male   \n",
            "12236  21.0       0.00        707.0  15642099.0        148564.76    Male   \n",
            "\n",
            "      Geography  HasCrCard  IsActiveMember  NumOfProducts    Surname  Tenure  \\\n",
            "7180     France        1.0             1.0            1.0       Mays     1.0   \n",
            "10393    France        1.0             0.0            1.0     Ch'eng     2.0   \n",
            "80      Germany        1.0             0.0            2.0      Ch'in     6.0   \n",
            "3365      Spain        0.0             1.0            2.0         K?     0.0   \n",
            "12236    France        1.0             1.0            2.0  Trevisani     3.0   \n",
            "\n",
            "          id  \n",
            "7180    7180  \n",
            "10393  10393  \n",
            "80        80  \n",
            "3365    3365  \n",
            "12236  12236  \n",
            "Train targets:\n",
            "7180     0.0\n",
            "10393    0.0\n",
            "80       0.0\n",
            "3365     0.0\n",
            "12236    0.0\n",
            "Name: Exited, dtype: float64\n",
            "Validation inputs:\n",
            "       Age    Balance  CreditScore  CustomerId  EstimatedSalary  Gender  \\\n",
            "6490  46.0  115764.32        714.0  15794345.0         72945.32    Male   \n",
            "3646  41.0       0.00        593.0  15617348.0         38196.24    Male   \n",
            "5306  38.0       0.00        731.0  15787907.0        116971.05  Female   \n",
            "652   43.0  155739.76        673.0  15803378.0        111622.76  Female   \n",
            "2627  30.0       0.00        678.0  15772423.0        143681.85  Female   \n",
            "\n",
            "     Geography  HasCrCard  IsActiveMember  NumOfProducts   Surname  Tenure  \\\n",
            "6490   Germany        1.0             1.0            4.0   Pirozzi     1.0   \n",
            "3646    France        1.0             1.0            2.0   Ritchie     5.0   \n",
            "5306    France        0.0             1.0            2.0     Hs?eh     2.0   \n",
            "652     France        0.0             1.0            1.0      Onio     4.0   \n",
            "2627     Spain        1.0             0.0            2.0  Genovesi     4.0   \n",
            "\n",
            "        id  \n",
            "6490  6490  \n",
            "3646  3646  \n",
            "5306  5306  \n",
            "652    652  \n",
            "2627  2627  \n",
            "Validation targets:\n",
            "6490    1.0\n",
            "3646    0.0\n",
            "5306    0.0\n",
            "652     0.0\n",
            "2627    0.0\n",
            "Name: Exited, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. В тренувальному та валідаційному наборі перетворіть категоріальні ознаки на тип `category`. Можна це зробити двома способами:\n",
        " 1. `df[col_name].astype('category')`, як було продемонстровано в лекції\n",
        " 2. використовуючи метод `pd.Categorical(df[col_name])`"
      ],
      "metadata": {
        "id": "cq0JU7MqHgp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = raw_df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_columns"
      ],
      "metadata": {
        "id": "UPmqo-Mr4yUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293c7522-d269-4f4c-c64f-5d5d523e6a18"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Surname', 'Geography', 'Gender']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_columns:\n",
        "    train_inputs[col] = pd.Categorical(train_inputs[col])\n",
        "    val_inputs[col] = pd.Categorical(val_inputs[col])\n",
        "\n",
        "# Print resulrs\n",
        "print(\"Train inputs:\")\n",
        "print(train_inputs.dtypes)\n",
        "print(\"Validation inputs:\")\n",
        "print(val_inputs.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shED2BOQp771",
        "outputId": "6a379199-63a7-4f19-a7ad-76d060845400"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train inputs:\n",
            "Age                 float64\n",
            "Balance             float64\n",
            "CreditScore         float64\n",
            "CustomerId          float64\n",
            "EstimatedSalary     float64\n",
            "Gender             category\n",
            "Geography          category\n",
            "HasCrCard           float64\n",
            "IsActiveMember      float64\n",
            "NumOfProducts       float64\n",
            "Surname            category\n",
            "Tenure              float64\n",
            "id                    int64\n",
            "dtype: object\n",
            "Validation inputs:\n",
            "Age                 float64\n",
            "Balance             float64\n",
            "CreditScore         float64\n",
            "CustomerId          float64\n",
            "EstimatedSalary     float64\n",
            "Gender             category\n",
            "Geography          category\n",
            "HasCrCard           float64\n",
            "IsActiveMember      float64\n",
            "NumOfProducts       float64\n",
            "Surname            category\n",
            "Tenure              float64\n",
            "id                    int64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Навчіть на отриманих даних модель `XGBoostClassifier`. Параметри алгоритму встановіть на свій розсуд, ми далі будемо їх тюнити. Рекомендую тренувати не дуже складну модель.\n",
        "\n",
        "  Опис всіх конфігураційних параметрів XGBoostClassifier - тут https://xgboost.readthedocs.io/en/stable/parameter.html#global-config\n",
        "\n",
        "  **Важливо:** зробіть такі налаштування `XGBoostClassifier` аби він самостійно обробляв незаповнені значення в даних і обробляв категоріальні колонки.\n",
        "\n",
        "  Можна також, якщо працюєте в Google Colab, увімкнути можливість використання GPU (`Runtime -> Change runtime type -> T4 GPU`) і встановити параметр `device='cuda'` в `XGBoostClassifier` для пришвидшення тренування бустинг моделі.\n",
        "  \n",
        "  Після тренування моделі\n",
        "  1. Виміряйте точність з допомогою AUROC на тренувальному та валідаційному наборах.\n",
        "  2. Зробіть висновок про отриману модель: вона хороша/погана, чи є high bias/high variance?\n",
        "  3. Порівняйте якість цієї моделі з тою, що ви отрмали з використанням DecisionTrees раніше. Чи вийшло покращити якість?"
      ],
      "metadata": {
        "id": "_LxWkv4o-wMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement XGBClassifier algorithm\n",
        "xgb_clf = XGBClassifier(\n",
        "    use_label_encoder=False,\n",
        "    missing=np.nan,\n",
        "    enable_categorical=True,\n",
        "    random_state=42,\n",
        "    device='cuda',\n",
        "    max_depth=3,\n",
        "    n_estimators=10\n",
        ")\n",
        "\n",
        "xgb_clf.fit(train_inputs, train_targets)\n",
        "\n",
        "# Predict on train and validation datasets\n",
        "train_preds = xgb_clf.predict_proba(train_inputs)[:, 1]\n",
        "val_preds = xgb_clf.predict_proba(val_inputs)[:, 1]\n",
        "\n",
        "# Compute AUROC for train and validation datasets\n",
        "train_auc = roc_auc_score(train_targets, train_preds)\n",
        "val_auc = roc_auc_score(val_targets, val_preds)\n",
        "\n",
        "# Print AUROC for train and validation datasets\n",
        "print(f\"AUROC for train dataset: {train_auc}\")\n",
        "print(f\"AUROC for validation dataset: {val_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZBH3ZRFtX2X",
        "outputId": "1ff94ef4-e0fe-434c-9e48-e0e61d61d2cc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC for train dataset: 0.9421334105891545\n",
            "AUROC for validation dataset: 0.9251563893271144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:38:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define train and validation inputs\n",
        "train_pred = xgb_clf.predict(train_inputs)\n",
        "val_pred = xgb_clf.predict(val_inputs)\n",
        "\n",
        "# Print classification reports\n",
        "print(classification_report(train_targets, train_pred, digits=4))\n",
        "print(classification_report(val_targets, val_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAf_GGPEuzb7",
        "outputId": "b1e9fd86-0ed8-4c29-d0a8-a16388358b02"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9162    0.9684    0.9416      9558\n",
            "         1.0     0.8408    0.6532    0.7352      2442\n",
            "\n",
            "    accuracy                         0.9042     12000\n",
            "   macro avg     0.8785    0.8108    0.8384     12000\n",
            "weighted avg     0.9008    0.9042    0.8996     12000\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9047    0.9653    0.9340      2390\n",
            "         1.0     0.8156    0.6016    0.6925       610\n",
            "\n",
            "    accuracy                         0.8913      3000\n",
            "   macro avg     0.8601    0.7835    0.8132      3000\n",
            "weighted avg     0.8866    0.8913    0.8849      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Використовуючи бібліотеку `Hyperopt` і приклад пошуку гіперпараметрів для `XGBoostClassifier` з лекції знайдіть оптимальні значення гіперпараметрів `XGBoostClassifier` для нашої задачі. Задайте свою сітку гіперпараметрів виходячи з тих параметрів, які ви б хотіли перебрати. Поставте кількість раундів в підборі гіперпараметрів рівну **20**.\n",
        "\n",
        "  **Увага!** Для того, аби скористатись hyperopt, нам треба задати функцію `objective`. В ній ми маємо задати loss - це може будь-яка метрика, але бажано використовувтаи ту, яка цільова в вашій задачі. Чим менший лосс - тим ліпша модель на думку hyperopt. Тож, тут нам треба задати loss - негативне значення AUROC. В лекції ми натомість використовували Accuracy.\n",
        "\n",
        "  Після успішного завершення пошуку оптимальних гіперпараметрів\n",
        "    - виведіть найкращі значення гіперпараметрів\n",
        "    - створіть в окремій зміній `final_clf` модель `XGBoostClassifier` з найкращими гіперпараметрами\n",
        "    - навчіть модель `final_clf`\n",
        "    - оцініть якість моделі `final_clf` на тренувальній і валідаційній вибірках з допомогою AUROC.\n",
        "    - зробіть висновок про якість моделі. Чи стала вона краще порівняно з попереднім пунктом (2) цього завдання?"
      ],
      "metadata": {
        "id": "U4hm5qYs_f7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective function for hyperparameter tuning\n",
        "def objective(params):\n",
        "    clf = xgb.XGBClassifier(\n",
        "        n_estimators=int(params['n_estimators']),\n",
        "        learning_rate=params['learning_rate'],\n",
        "        max_depth=int(params['max_depth']),\n",
        "        min_child_weight=params['min_child_weight'],\n",
        "        subsample=params['subsample'],\n",
        "        colsample_bytree=params['colsample_bytree'],\n",
        "        gamma=params['gamma'],\n",
        "        reg_alpha=params['reg_alpha'],\n",
        "        reg_lambda=params['reg_lambda'],\n",
        "        enable_categorical=True,  # Enable categorical handling\n",
        "        use_label_encoder=False,\n",
        "        tree_method='gpu_hist' if params.get('device') == 'cuda' else 'hist',  # Use GPU if available\n",
        "        eval_metric='auc'  # Evaluation metric is now specified here\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    clf.fit(\n",
        "        train_inputs,\n",
        "        train_targets,\n",
        "        eval_set=[(val_inputs, val_targets)],  # Validation set for evaluation\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Predict probabilities\n",
        "    pred = clf.predict_proba(val_inputs)[:, 1]\n",
        "\n",
        "    # Calculate validation AUC\n",
        "    auc = roc_auc_score(val_targets, pred)\n",
        "\n",
        "    # Return the negative AUC as Hyperopt minimizes the objective\n",
        "    return {'loss': -auc, 'status': STATUS_OK}\n",
        "\n",
        "# Define the parameter search space\n",
        "space = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 500, 25),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "    'max_depth': hp.quniform('max_depth', 3, 15, 1),\n",
        "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
        "    'device': hp.choice('device', ['cuda', 'cpu'])  # Use GPU or CPU\n",
        "}\n",
        "\n",
        "# Perform optimization\n",
        "trials = Trials()\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
        "\n",
        "# Convert integer parameters back to int\n",
        "best['n_estimators'] = int(best['n_estimators'])\n",
        "best['max_depth'] = int(best['max_depth'])\n",
        "best['min_child_weight'] = int(best['min_child_weight'])\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best hyperparameters: \", best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVx9YvxJyn21",
        "outputId": "77e08e20-44e3-4f32-8517-41084078bbd2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:42:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:42:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  5%|▌         | 1/20 [00:04<01:17,  4.07s/trial, best loss: -0.9248391522052267]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 10%|█         | 2/20 [00:06<00:54,  3.03s/trial, best loss: -0.9248391522052267]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 15%|█▌        | 3/20 [00:07<00:39,  2.30s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 20%|██        | 4/20 [00:19<01:34,  5.91s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:15] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 25%|██▌       | 5/20 [00:24<01:22,  5.50s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 30%|███       | 6/20 [00:25<00:58,  4.19s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:21] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 35%|███▌      | 7/20 [00:27<00:43,  3.32s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 40%|████      | 8/20 [00:31<00:44,  3.75s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 45%|████▌     | 9/20 [00:35<00:40,  3.64s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 50%|█████     | 10/20 [00:36<00:29,  2.91s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:32] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 55%|█████▌    | 11/20 [00:37<00:20,  2.28s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 60%|██████    | 12/20 [00:37<00:13,  1.74s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 65%|██████▌   | 13/20 [00:40<00:13,  1.92s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 70%|███████   | 14/20 [00:40<00:09,  1.55s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 75%|███████▌  | 15/20 [00:42<00:07,  1.52s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 80%|████████  | 16/20 [00:43<00:05,  1.37s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 85%|████████▌ | 17/20 [00:44<00:04,  1.41s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 90%|█████████ | 18/20 [00:46<00:02,  1.41s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 95%|█████████▌| 19/20 [00:47<00:01,  1.42s/trial, best loss: -0.9349677618492351]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:43:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 20/20 [00:48<00:00,  2.44s/trial, best loss: -0.9349677618492351]\n",
            "Best hyperparameters:  {'colsample_bytree': 0.596204237050998, 'device': 1, 'gamma': 0.2831238052385673, 'learning_rate': 0.06803284752535987, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 100, 'reg_alpha': 0.17656031881168488, 'reg_lambda': 0.18740985864864013, 'subsample': 0.8900034320657852}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zepiduxT1VXl",
        "outputId": "94ee7e9e-7fdf-405b-f345-4986b2bad31d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'colsample_bytree': 0.596204237050998,\n",
              " 'device': 1,\n",
              " 'gamma': 0.2831238052385673,\n",
              " 'learning_rate': 0.06803284752535987,\n",
              " 'max_depth': 3,\n",
              " 'min_child_weight': 2,\n",
              " 'n_estimators': 100,\n",
              " 'reg_alpha': 0.17656031881168488,\n",
              " 'reg_lambda': 0.18740985864864013,\n",
              " 'subsample': 0.8900034320657852}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the final model with the best hyperparameters\n",
        "final_clf = xgb.XGBClassifier(\n",
        "    n_estimators=best['n_estimators'],\n",
        "    learning_rate=best['learning_rate'],\n",
        "    max_depth=best['max_depth'],\n",
        "    min_child_weight=best['min_child_weight'],\n",
        "    subsample=best['subsample'],\n",
        "    colsample_bytree=best['colsample_bytree'],\n",
        "    gamma=best['gamma'],\n",
        "    reg_alpha=best['reg_alpha'],\n",
        "    reg_lambda=best['reg_lambda'],\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    missing=np.nan,\n",
        "    tree_method='hist',  # Use CPU (or 'gpu_hist' for GPU if applicable)\n",
        "    eval_metric=\"auc\"  # Built-in evaluation metric\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "final_clf.fit(\n",
        "    train_inputs,\n",
        "    train_targets,\n",
        "    eval_set=[(val_inputs, val_targets)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Predict probabilities for train and validation datasets\n",
        "train_preds_proba = final_clf.predict_proba(train_inputs)[:, 1]\n",
        "val_preds_proba = final_clf.predict_proba(val_inputs)[:, 1]\n",
        "\n",
        "# Calculate AUROC for train and validation datasets\n",
        "train_auc = roc_auc_score(train_targets, train_preds_proba)\n",
        "val_auc = roc_auc_score(val_targets, val_preds_proba)\n",
        "\n",
        "print(f\"Train AUROC: {train_auc:.4f}\")\n",
        "print(f\"Validation AUROC: {val_auc:.4f}\")\n",
        "\n",
        "# Predict class labels for train and validation datasets\n",
        "train_preds = final_clf.predict(train_inputs)\n",
        "val_preds = final_clf.predict(val_inputs)\n",
        "\n",
        "# Print classification reports\n",
        "print(\"Classification Report (Train):\")\n",
        "print(classification_report(train_targets, train_preds, digits=4))\n",
        "\n",
        "print(\"Classification Report (Validation):\")\n",
        "print(classification_report(val_targets, val_preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFrvk3-G2QWv",
        "outputId": "d9101717-025d-49a6-a44d-a58fb5076ccf"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:59:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train AUROC: 0.9472\n",
            "Validation AUROC: 0.9350\n",
            "Classification Report (Train):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9226    0.9669    0.9443      9558\n",
            "         1.0     0.8406    0.6826    0.7534      2442\n",
            "\n",
            "    accuracy                         0.9091     12000\n",
            "   macro avg     0.8816    0.8248    0.8489     12000\n",
            "weighted avg     0.9059    0.9091    0.9054     12000\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9217    0.9502    0.9357      2390\n",
            "         1.0     0.7780    0.6836    0.7277       610\n",
            "\n",
            "    accuracy                         0.8960      3000\n",
            "   macro avg     0.8498    0.8169    0.8317      3000\n",
            "weighted avg     0.8925    0.8960    0.8934      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "Overall Performance:\n",
        "\n",
        "Last model (with best hyperparameters) performs slightly better in both train and validation sets, as evidenced by its higher AUROC and weighted average F1-scores.\n",
        "\n",
        "Class Imbalance Handling:\n",
        "\n",
        "Last model achieves better recall and F1-scores for the minority class (Class 1) on both the training and validation sets compared to original model. This indicates that Model 1 is better at handling the minority class.\n",
        "\n",
        "Precision for Class 0:\n",
        "\n",
        "Both models have high precision for Class 0, but last model is marginally better.\n",
        "\n",
        "Recall for Class 1:\n",
        "\n",
        "Model 1 has significantly higher recall for the minority class (Train: 0.6826 vs. 0.6016; Validation: 0.6532 vs. 0.6016). This suggests last model is better at identifying positive cases.\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "Last model achieves higher accuracy on both the training and validation sets (Train: 0.9091 vs. 0.8913; Validation: 0.9042 vs. 0.8913).\n",
        "\n",
        "Weighted F1-Score:\n",
        "\n",
        "Last model has a higher weighted F1-score on both train and validation sets, demonstrating superior overall performance.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Last model (with best hyperparameters) is the better-performing model. It achieves higher AUROC, weighted F1-scores, and better recall for the minority class, making it more suitable for imbalanced datasets.\n",
        "Original model, while still reasonable, falls short, particularly in handling the minority class, which is critical in imbalanced classification problems."
      ],
      "metadata": {
        "id": "4grA9Stq3TBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Навчіть на наших даних модель LightGBM. Параметри алгоритму встановіть на свій розсуд, ми далі будемо їх тюнити. Рекомендую тренувати не дуже складну модель.\n",
        "\n",
        "  Опис всіх конфігураційних параметрів LightGBM - тут https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
        "\n",
        "  **Важливо:** зробіть такі налаштування LightGBM аби він самостійно обробляв незаповнені значення в даних і обробляв категоріальні колонки.\n",
        "\n",
        "  Аби передати категоріальні колонки в LightGBM - необхідно виявити їх індекси і передати в параметрі `cat_feature=cat_feature_indexes`\n",
        "\n",
        "  Після тренування моделі\n",
        "  1. Виміряйте точність з допомогою AUROC на тренувальному та валідаційному наборах.\n",
        "  2. Зробіть висновок про отриману модель: вона хороша/погана, чи є high bias/high variance?\n",
        "  3. Порівняйте якість цієї моделі з тою, що ви отрмали з використанням XGBoostClassifier раніше. Чи вийшло покращити якість?"
      ],
      "metadata": {
        "id": "Vg77SVWrBBmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "print(lgb.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjnDny3B1JK6",
        "outputId": "fb855891-9839-41b4-fc22-9dbc9d47a200"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_feature_indexes = [train_inputs.columns.get_loc(col) for col in categorical_columns]"
      ],
      "metadata": {
        "id": "C-9aZn4d45No"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "# Define the LightGBM Classifier with specified hyperparameters\n",
        "lgb_clf = lgb.LGBMClassifier(\n",
        "    max_depth=3,\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    categorical_feature=cat_feature_indexes,  # For recognizing categorical features\n",
        "    missing=np.nan,  # Explicitly specify missing values\n",
        "    device_type='cpu'  # Explicitly set the device type (change to 'gpu' if GPU is available)\n",
        ")\n",
        "\n",
        "# Train the model with a validation set using callbacks\n",
        "lgb_clf.fit(\n",
        "    train_inputs,\n",
        "    train_targets,\n",
        "    eval_set=[(val_inputs, val_targets)],  # Validation set for evaluation\n",
        "    eval_metric='auc',  # Use AUC for evaluation\n",
        "    callbacks=[\n",
        "        early_stopping(stopping_rounds=10),  # Stop if no improvement after 10 rounds\n",
        "        log_evaluation(period=10)  # Log evaluation metrics every 10 rounds\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Predict probabilities for train and validation datasets\n",
        "train_preds_proba = lgb_clf.predict_proba(train_inputs)[:, 1]\n",
        "val_preds_proba = lgb_clf.predict_proba(val_inputs)[:, 1]\n",
        "\n",
        "# Calculate AUROC for train and validation datasets\n",
        "train_auc = roc_auc_score(train_targets, train_preds_proba)\n",
        "val_auc = roc_auc_score(val_targets, val_preds_proba)\n",
        "\n",
        "print(f\"Train AUROC: {train_auc:.4f}\")\n",
        "print(f\"Validation AUROC: {val_auc:.4f}\")\n",
        "\n",
        "# Predict class labels for train and validation datasets\n",
        "train_preds = lgb_clf.predict(train_inputs)\n",
        "val_preds = lgb_clf.predict(val_inputs)\n",
        "\n",
        "# Print classification reports\n",
        "print(\"Classification Report (Train):\")\n",
        "print(classification_report(train_targets, train_preds, digits=4))\n",
        "\n",
        "print(\"Classification Report (Validation):\")\n",
        "print(classification_report(val_targets, val_preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht0UgLs_49mG",
        "outputId": "e1394e37-5ad3-4d46-b1f0-4ef26f1bf0f5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001911 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1826\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[10]\tvalid_0's auc: 0.916222\tvalid_0's binary_logloss: 0.325978\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[20]\tvalid_0's auc: 0.925521\tvalid_0's binary_logloss: 0.28474\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[30]\tvalid_0's auc: 0.932424\tvalid_0's binary_logloss: 0.266021\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[40]\tvalid_0's auc: 0.934474\tvalid_0's binary_logloss: 0.257271\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[50]\tvalid_0's auc: 0.935524\tvalid_0's binary_logloss: 0.252418\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[49]\tvalid_0's auc: 0.935641\tvalid_0's binary_logloss: 0.252408\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "Train AUROC: 0.9496\n",
            "Validation AUROC: 0.9356\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "Classification Report (Train):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9241    0.9689    0.9460      9558\n",
            "         1.0     0.8498    0.6884    0.7606      2442\n",
            "\n",
            "    accuracy                         0.9118     12000\n",
            "   macro avg     0.8870    0.8286    0.8533     12000\n",
            "weighted avg     0.9090    0.9118    0.9083     12000\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9187    0.9594    0.9386      2390\n",
            "         1.0     0.8075    0.6672    0.7307       610\n",
            "\n",
            "    accuracy                         0.9000      3000\n",
            "   macro avg     0.8631    0.8133    0.8347      3000\n",
            "weighted avg     0.8961    0.9000    0.8963      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "**Strengths of LightGBM model:**\n",
        "\n",
        "1. High AUROC: Both Train and Validation AUROC scores are very high (0.9496 and 0.9350), showing strong discriminatory power between classes.\n",
        "2. Good Performance on Majority Class:\n",
        "For Class 0, precision, recall, and F1-scores are consistently high across both train and validation datasets.\n",
        "3. Generalization:\n",
        "The validation performance is close to the training performance, indicating that the model generalizes well without overfitting.\n",
        "\n",
        "**Weaknesses of LightGBM model:**\n",
        "1. Class Imbalance Impact:\n",
        "The minority class (Class 1) has noticeably lower recall (Train: 0.6884, Validation: 0.6572), indicating that the model struggles to identify some positive cases.\n",
        "This issue is common in imbalanced datasets and affects the F1-score for the minority class.\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "1. The model performs exceptionally well for the majority class and achieves high overall AUROC and weighted F1-scores, making it a reliable model for general tasks.\n",
        "2. However, the recall for the minority class is lower, which could be problematic if the task requires high sensitivity for Class 1.\n",
        "3. To improve minority class performance, techniques like class weighting, oversampling (e.g., SMOTE), or advanced algorithms designed to handle imbalanced datasets could be considered.\n",
        "\n",
        "\n",
        "**Recommendations:**\n",
        "1. Fine-tune for Minority Class:\n",
        "Use class_weight='balanced' or custom weights in the LightGBM classifier to penalize errors on the minority class.\n",
        "2. Additional Feature Engineering:\n",
        "Add or transform features to better distinguish between the two classes, potentially improving recall for the minority class.\n",
        "3. Resampling Techniques:\n",
        "Oversample the minority class using SMOTE or other techniques to provide the model with more balanced training data.\n",
        "4. Hyperparameter Optimization:\n",
        "Perform grid search or randomized search to fine-tune hyperparameters further, especially those affecting regularization (e.g., reg_alpha, reg_lambda)."
      ],
      "metadata": {
        "id": "JvRl1YnM5bVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Використовуючи бібліотеку `Hyperopt` і приклад пошуку гіперпараметрів для `LightGBM` з лекції знайдіть оптимальні значення гіперпараметрів `LightGBM` для нашої задачі. Задайте свою сітку гіперпараметрів виходячи з тих параметрів, які ви б хотіли перебрати. Поставте кількість раундів в підборі гіперпараметрів рівну **10**.\n",
        "\n",
        "  **Увага!** Для того, аби скористатись hyperopt, нам треба задати функцію `objective`. І тут ми також ставимо loss - негативне значення AUROC, як і при пошуці гіперпараметрів для XGBoost. До речі, можна спробувати написати код так, аби в objective передавати лише модель і не писати схожий код двічі :)\n",
        "\n",
        "  Після успішного завершення пошуку оптимальних гіперпараметрів\n",
        "    - виведіть найкращі значення гіперпараметрів\n",
        "    - створіть в окремій зміній `final_lgb_clf` модель `LightGBM` з найкращими гіперпараметрами\n",
        "    - навчіть модель `final_lgb_clf`\n",
        "    - оцініть якість моделі `final_lgb_clf` на тренувальній і валідаційній вибірках з допомогою AUROC.\n",
        "    - зробіть висновок про якість моделі. Чи стала вона краще порівняно з попереднім пунктом (4) цього завдання?"
      ],
      "metadata": {
        "id": "nCnkGD_sEW1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function\n",
        "def objective(params, train_inputs, train_targets, val_inputs, val_targets):\n",
        "    # Ensure num_leaves >= 2 ** max_depth\n",
        "    params['num_leaves'] = max(int(params['num_leaves']), 2 ** int(params['max_depth']))\n",
        "\n",
        "    clf = lgb.LGBMClassifier(\n",
        "        n_estimators=int(params['n_estimators']),\n",
        "        learning_rate=params['learning_rate'],\n",
        "        max_depth=int(params['max_depth']),\n",
        "        num_leaves=int(params['num_leaves']),\n",
        "        min_child_weight=params['min_child_weight'],\n",
        "        subsample=params['subsample'],\n",
        "        colsample_bytree=params['colsample_bytree'],\n",
        "        reg_alpha=params['reg_alpha'],\n",
        "        reg_lambda=params['reg_lambda'],\n",
        "        max_bin=int(params['max_bin']),\n",
        "        categorical_feature=params['cat_feature'],\n",
        "        missing=np.nan  # Explicitly handle missing values\n",
        "    )\n",
        "\n",
        "    clf.fit(\n",
        "        train_inputs,\n",
        "        train_targets,\n",
        "        eval_set=[(val_inputs, val_targets)],\n",
        "        eval_metric='auc',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # Predict probabilities and compute validation AUC\n",
        "    val_preds = clf.predict_proba(val_inputs)[:, 1]\n",
        "    auc = roc_auc_score(val_targets, val_preds)\n",
        "\n",
        "    return {'loss': -auc, 'status': STATUS_OK}\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_space = {\n",
        "    'num_leaves': hp.choice('num_leaves', range(31, 151, 10)),\n",
        "    'max_depth': hp.choice('max_depth', range(0, 15, 1)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', -3, 0),  # log-uniform for smaller values\n",
        "    'n_estimators': hp.choice('n_estimators', range(50, 501, 50)),\n",
        "    'min_child_weight': hp.uniform('min_child_weight', 1, 10),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
        "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
        "    'max_bin': hp.choice('max_bin', range(100, 256, 10)),\n",
        "    'cat_feature': hp.choice('cat_feature', [cat_feature_indexes])  # Pass categorical feature indexes\n",
        "}\n",
        "\n",
        "# Run optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(\n",
        "    fn=lambda params: objective(params, train_inputs, train_targets, val_inputs, val_targets),\n",
        "    space=param_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=10,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "# Convert hyperparameters to correct types\n",
        "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
        "best_params['max_depth'] = int(best_params['max_depth'])\n",
        "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "best_params['max_bin'] = int(best_params['max_bin'])\n",
        "\n",
        "# Ensure num_leaves >= 2 ** max_depth\n",
        "best_params['num_leaves'] = max(best_params['num_leaves'], 2 ** best_params['max_depth'])\n",
        "\n",
        "print(\"Best Hyperparameters: \", best_params)\n"
      ],
      "metadata": {
        "id": "cfMQKA4D47Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a396a12e-7222-4224-8c94-3bd98d61f4d2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002443 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1706\n",
            "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001505 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1157\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 10%|█         | 1/10 [00:01<00:12,  1.42s/trial, best loss: -0.9329717401742231]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002789 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1656\n",
            " 20%|██        | 2/10 [00:02<00:08,  1.03s/trial, best loss: -0.9337368818163111]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 20%|██        | 2/10 [00:02<00:08,  1.03s/trial, best loss: -0.9337368818163111]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003718 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1407\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 30%|███       | 3/10 [00:04<00:10,  1.56s/trial, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            " 40%|████      | 4/10 [00:05<00:07,  1.26s/trial, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1407\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 40%|████      | 4/10 [00:05<00:07,  1.26s/trial, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004932 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1606\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 50%|█████     | 5/10 [00:06<00:05,  1.04s/trial, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003578 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1207\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 60%|██████    | 6/10 [00:06<00:03,  1.07trial/s, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001410 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 70%|███████   | 7/10 [00:07<00:02,  1.35trial/s, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001335 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1257\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            " 80%|████████  | 8/10 [00:07<00:01,  1.41trial/s, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Warning] categorical_feature is set=10,6,5, categorical_column=5,6,10 will be ignored. Current value: categorical_feature=10,6,5\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004268 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1556\n",
            " 90%|█████████ | 9/10 [00:09<00:01,  1.15s/trial, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            " 90%|█████████ | 9/10 [00:10<00:01,  1.15s/trial, best loss: -0.9377824267782428]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2118: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:2140: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f\"{cat_alias} in param dict is overridden.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Unknown parameter: missing\n",
            "100%|██████████| 10/10 [00:12<00:00,  1.26s/trial, best loss: -0.9377824267782428]\n",
            "Best Hyperparameters:  {'cat_feature': 0, 'colsample_bytree': 0.6123904867560698, 'learning_rate': 0.08627631198105155, 'max_bin': 12, 'max_depth': 14, 'min_child_weight': 9.965336896438188, 'n_estimators': 9, 'num_leaves': 16384, 'reg_alpha': 0.6135623503834426, 'reg_lambda': 0.5995539203034128, 'subsample': 0.9677170631342039}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure hyperparameters are logical\n",
        "if best_params['n_estimators'] < 10:\n",
        "    best_params['n_estimators'] = 50\n",
        "if best_params['max_bin'] <= 0:\n",
        "    best_params['max_bin'] = 255\n",
        "\n",
        "# Create the final LightGBM model with the best hyperparameters\n",
        "final_lgb_clf = lgb.LGBMClassifier(\n",
        "    num_leaves=best_params['num_leaves'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    min_child_weight=best_params['min_child_weight'],\n",
        "    subsample=best_params['subsample'],\n",
        "    colsample_bytree=best_params['colsample_bytree'],\n",
        "    reg_alpha=best_params['reg_alpha'],\n",
        "    reg_lambda=best_params['reg_lambda'],\n",
        "    max_bin=best_params['max_bin']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "final_lgb_clf.fit(train_inputs, train_targets)\n",
        "\n",
        "# Evaluate on the training set\n",
        "train_pred_proba = final_lgb_clf.predict_proba(train_inputs)[:, 1]\n",
        "train_pred = (train_pred_proba > 0.5).astype(int)\n",
        "train_roc_auc = roc_auc_score(train_targets, train_pred_proba)\n",
        "\n",
        "# Evaluate on the validation set\n",
        "val_pred_proba = final_lgb_clf.predict_proba(val_inputs)[:, 1]\n",
        "val_pred = (val_pred_proba > 0.5).astype(int)\n",
        "val_roc_auc = roc_auc_score(val_targets, val_pred_proba)\n",
        "\n",
        "# Print AUROC scores\n",
        "print(f\"Train AUROC: {train_roc_auc:.4f}\")\n",
        "print(f\"Validation AUROC: {val_roc_auc:.4f}\")\n",
        "\n",
        "# Classification report for the training set\n",
        "print(\"\\nClassification Report (Train):\")\n",
        "print(classification_report(train_targets, train_pred, digits=4))\n",
        "\n",
        "# Classification report for the validation set\n",
        "print(\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(val_targets, val_pred, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH-9XGXk7uEb",
        "outputId": "0b280613-e80d-4071-c5b3-8f3089262fa0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
            "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
            "[LightGBM] [Info] Number of positive: 2442, number of negative: 9558\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001489 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 572\n",
            "[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 13\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203500 -> initscore=-1.364561\n",
            "[LightGBM] [Info] Start training from score -1.364561\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Train AUROC: 0.9657\n",
            "Validation AUROC: 0.9367\n",
            "\n",
            "Classification Report (Train):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9363    0.9710    0.9534      9558\n",
            "         1.0     0.8673    0.7416    0.7996      2442\n",
            "\n",
            "    accuracy                         0.9243     12000\n",
            "   macro avg     0.9018    0.8563    0.8765     12000\n",
            "weighted avg     0.9223    0.9243    0.9221     12000\n",
            "\n",
            "\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9193    0.9531    0.9359      2390\n",
            "         1.0     0.7854    0.6721    0.7244       610\n",
            "\n",
            "    accuracy                         0.8960      3000\n",
            "   macro avg     0.8524    0.8126    0.8301      3000\n",
            "weighted avg     0.8921    0.8960    0.8929      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "**Train dataset:**\n",
        "1. The model achieved an AUROC of 0.9657 on the training dataset, indicating excellent discriminatory power.\n",
        "2. The model performs better on Class 0 compared to Class 1, likely due to class imbalance.\n",
        "3. Overall training accuracy is 92.43%, which is high and shows that the model fits the training data well.\n",
        "4. The weighted average F1-Score is 0.9221, reflecting good performance while accounting for class imbalance.\n",
        "5. The macro average F1-Score is slightly lower at 0.8765, showing reduced performance on the minority class (Class 1).\n",
        "\n",
        "**Validation dataset:**\n",
        "1. The model achieved an AUROC of 0.9367 on the validation dataset, showing that it generalizes well to unseen data.\n",
        "2. Similar to the training dataset, the model struggles to classify the minority class (Class 1), with a noticeable drop in recall (67.21%) and F1-Score (72.44%).\n",
        "3. The overall validation accuracy is 89.60%, slightly lower than the training accuracy, which indicates good generalization with limited overfitting.\n",
        "4. The weighted average F1-Score is 0.8929, which is consistent with the high AUROC.\n",
        "5. The macro average F1-Score is 0.8301, showing lower performance on Class 1.\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "1. Overall performance: The LightGBM classifier demonstrates strong performance on both the training and validation datasets, achieving high AUROC values (0.9657 on training and 0.9367 on validation).\n",
        "The slight difference between training and validation performance suggests the model generalizes well without significant overfitting.\n",
        "2. Class imbalance impact: The model performs better on the majority class (Class 0) compared to the minority class (Class 1). This is evident in the lower recall and F1-Score for Class 1.\n",
        "The imbalance affects the overall macro average F1-Score, particularly in the validation dataset.\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "To improve performance on the minority class (Class 1), we can consider using techniques like:\n",
        "\n",
        "1. Adjusting the decision threshold for class probabilities.\n",
        "2. Trying cost-sensitive learning (e.g., adding class weights to penalize misclassifications of the minority class).\n",
        "3. Experimenting with oversampling (e.g., SMOTE) or undersampling to balance the dataset.\n",
        "\n",
        "**Final assessment:**\n",
        "\n",
        "The model is well-suited for this dataset, but improvements in recall and F1-Score for Class 1 should be prioritized to enhance its overall effectiveness in real-world applications."
      ],
      "metadata": {
        "id": "yCtv-6UH7_Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Оберіть модель з експериментів в цьому ДЗ і зробіть новий `submission` на Kaggle та додайте код для цього і скріншот скора на публічному лідерборді.\n",
        "  \n",
        "  **Напишіть коментар, чому ви обрали саме цю модель?**\n",
        "\n",
        "  І я вас вітаю - це останнє завдання з цим набором даних 💪 На цьому етапі корисно проаналізувати, які моделі показали себе найкраще і подумати, чому."
      ],
      "metadata": {
        "id": "XArADR2CG8VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "The LightGBM model after hyperparameter optimization (Task 5) is well-suited for this dataset because it shows the best results on the validation datasets with the highest AUROC value and well-balanced precision and recall values for both classes. This model has the best overall performance and demonstrates that hyperparameter optimization significantly improved its ability to distinguish between classes."
      ],
      "metadata": {
        "id": "-Ac2P2SL-qIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "test_df = pd.read_csv('./bank-customer-churn-prediction-dlu/test.csv')\n",
        "\n",
        "# Ensure test data has the same columns as train data\n",
        "train_columns = train_inputs.columns.tolist()\n",
        "\n",
        "for col in train_columns:\n",
        "    if col not in test_df.columns:\n",
        "        # Fill missing columns with a reasonable default\n",
        "        test_df[col] = 0  # or other default based on context\n",
        "\n",
        "# Ensure the column order matches\n",
        "test_inputs = test_df[train_columns]\n",
        "\n",
        "# Handle categorical columns to align with train data\n",
        "for col in categorical_columns:\n",
        "    if col in test_inputs.columns:  # Check if the column exists in the test data\n",
        "        test_inputs[col] = pd.Categorical(\n",
        "            test_inputs[col],\n",
        "            categories=train_inputs[col].cat.categories  # Use train data categories\n",
        "        )\n",
        "\n",
        "# Predictions using the final LightGBM model\n",
        "test_preds = final_lgb_clf.predict_proba(test_inputs)[:, 1]\n",
        "\n",
        "# Prepare submission file\n",
        "if 'id' not in test_df.columns:\n",
        "    test_df['id'] = range(len(test_df))  # Add default IDs if missing\n",
        "\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_df['id'],       # Ensure ID column exists\n",
        "    'Exited': test_preds       # Use model predictions\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission_path = 'drive/MyDrive/Colab Notebooks/data/submission_light_opt.csv'\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"Submission file saved to {submission_path}\")\n"
      ],
      "metadata": {
        "id": "COIjJH9f5SSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7353f4-10bf-43ab-f316-153c87a36b6c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved to drive/MyDrive/Colab Notebooks/data/submission_light_opt.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WX-fVVCN_HKl",
        "outputId": "69b1c175-da6b-4044-d984-5e0b6142f111"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id    Exited\n",
              "0  15000  0.053399\n",
              "1  15001  0.021365\n",
              "2  15002  0.044127\n",
              "3  15003  0.523982\n",
              "4  15004  0.023533"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6399a0e2-ec33-4049-b80b-533fdcaee57c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15000</td>\n",
              "      <td>0.053399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15001</td>\n",
              "      <td>0.021365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15002</td>\n",
              "      <td>0.044127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15003</td>\n",
              "      <td>0.523982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15004</td>\n",
              "      <td>0.023533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6399a0e2-ec33-4049-b80b-533fdcaee57c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6399a0e2-ec33-4049-b80b-533fdcaee57c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6399a0e2-ec33-4049-b80b-533fdcaee57c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fa3b4458-9c76-4fcf-b05b-9812fc3e84e5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fa3b4458-9c76-4fcf-b05b-9812fc3e84e5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fa3b4458-9c76-4fcf-b05b-9812fc3e84e5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "submission_df",
              "summary": "{\n  \"name\": \"submission_df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2886,\n        \"min\": 15000,\n        \"max\": 24999,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          21252,\n          19684,\n          16731\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Exited\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28437802274993856,\n        \"min\": 0.005876018467909288,\n        \"max\": 0.9798216880207294,\n        \"num_unique_values\": 9990,\n        \"samples\": [\n          0.03240223080110372,\n          0.9091082852568578,\n          0.9377052173766591\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}
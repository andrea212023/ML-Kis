{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrea212023/ML-Kis/blob/main/HW_Intro_to_neural_networks_KIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to tensors\n",
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "targets_tensor = torch.from_numpy(targets)\n",
        "\n",
        "# Displaying the result\n",
        "print(\"Inputs Tensor:\", inputs_tensor)\n",
        "print(\"Targets Tensor:\", targets_tensor)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4f2aa0-263c-4b11-cde8-39c1bbdb3086"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs Tensor: tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "Targets Tensor: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the random seed for reproducibility\n",
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bca2b35-2476-43aa-a076-c092853a6e77"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c1806c82f70>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming inputs have 3 features and we have 1 output (logistic regression)\n",
        "\n",
        "# Initialize weights (w) and bias (b)\n",
        "input_size = 3  # Number of input features\n",
        "output_size = 1  # Binary output for logistic regression\n",
        "\n",
        "w = torch.randn(input_size, output_size, requires_grad=True)\n",
        "b = torch.randn(output_size, requires_grad=True)\n",
        "\n",
        "print(\"Weights (w):\", w)\n",
        "print(\"Bias (b):\", b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89ad715-d48b-4e08-f37b-23dfcdae0841"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights (w): tensor([[0.6614],\n",
            "        [0.2669],\n",
            "        [0.0617]], requires_grad=True)\n",
            "Bias (b): tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the hypothesis for logistic regression\n",
        "def model(x, w, b):\n",
        "    # Calculate the linear combination (x * w + b)\n",
        "    linear_output = torch.matmul(x, w) + b\n",
        "\n",
        "    # Apply the  function to get the logistic regression prediction\n",
        "    y_pred = 1 / (1 + torch.exp(-linear_output))\n",
        "    return y_pred\n",
        "\n",
        "# Calculate predictions using the model\n",
        "predictions  = model(inputs_tensor, w, b)\n",
        "\n",
        "# Output the predictions\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfba395-35c5-430f-8031-39c49e868b38"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why this happens?**\n",
        "\n",
        "The values in the linear_output tensor are large positive numbers because the dot product x⋅w + b has resulted in these high values, likely due to one or more of the following reasons:\n",
        "\n",
        "1. Large Magnitudes of Weights or Biases (w and b):\n",
        "If the weights (w) and bias (b) were initialized with high values, they can amplify the dot product result.\n",
        "2. Unnormalized Input Data (inputs_tensor):\n",
        "If the input data contains large values and has not been normalized, the multiplication with weights leads to excessively large outputs.\n",
        "3. Combination of Input and Weights:\n",
        "Even if either w or x alone is not large, their combination (dot product) may still result in high values due to their interaction.\n",
        "\n",
        "As a result, the sigmoid function applied to these high positive values outputs probabilities of 1 for all inputs.\n",
        "\n",
        "**Why Saturation is Problematic?**\n",
        "\n",
        "The predictions of 1, 1, 1, 1 are problematic because they resulted from saturation of the sigmoid function due to extremely large positive linear outputs.\n",
        "\n",
        "When the predictions are all 1, the model cannot distinguish between different input samples. This leads to several issues:\n",
        "\n",
        "1. No gradient for learning\n",
        "2. Loss function becomes ineffective\n",
        "3. Lack of meaningful predictions\n"
      ],
      "metadata": {
        "id": "_coxLJ83Ruxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_output = torch.matmul(inputs_tensor, w) + b\n",
        "print(\"Linear Output:\", linear_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD2vlT74QKw5",
        "outputId": "b9455e0a-7cad-4a0e-872f-d11ebba47ba4"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Output: tensor([[69.4361],\n",
            "        [88.2410],\n",
            "        [97.5041],\n",
            "        [81.8390],\n",
            "        [76.1967]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the Input Data\n",
        "print(\"Inputs Tensor Stats:\")\n",
        "print(\"Max Value:\", inputs_tensor.max())\n",
        "print(\"Min Value:\", inputs_tensor.min())\n",
        "print(\"Mean Value:\", inputs_tensor.mean())\n",
        "print(\"Standard Deviation:\", inputs_tensor.std())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUfQuxMzQ9ft",
        "outputId": "1734fd7f-1a5f-47ef-906a-6543dfbe08dd"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs Tensor Stats:\n",
            "Max Value: tensor(134.)\n",
            "Min Value: tensor(37.)\n",
            "Mean Value: tensor(74.8000)\n",
            "Standard Deviation: tensor(25.7049)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the Input Data\n",
        "inputs_tensor = (inputs_tensor - inputs_tensor.mean(dim=0)) / inputs_tensor.std(dim=0)"
      ],
      "metadata": {
        "id": "nuhP4TSPRADE"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the Weights and Biases (w and b)\n",
        "print(\"Weights:\", w)\n",
        "print(\"Bias:\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNOsZGNERDPU",
        "outputId": "271eabed-7f6f-4055-a834-75f93d5abad1"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: tensor([[0.6614],\n",
            "        [0.2669],\n",
            "        [0.0617]], requires_grad=True)\n",
            "Bias: tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize Weights and Biases\n",
        "torch.nn.init.xavier_uniform_(w)\n",
        "b.data.fill_(0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRgg3i7cRF44",
        "outputId": "366b9aad-9120-4123-9767-ebab12eeb6bc"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale the linear output\n",
        "linear_output = torch.clamp(torch.matmul(inputs_tensor, w) + b, min=-10, max=10)\n",
        "print(\"Clamped Linear Output:\", linear_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAMUEGqBRJk_",
        "outputId": "7f1752ef-63e9-4c34-cc6c-9602a397d49d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clamped Linear Output: tensor([[-0.3390],\n",
            "        [ 0.3061],\n",
            "        [-0.0933],\n",
            "        [-0.0127],\n",
            "        [ 0.1388]], grad_fn=<ClampBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate predictions\n",
        "predictions = 1 / (1 + torch.exp(-linear_output))\n",
        "print(\"New Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppD2maqLRMvC",
        "outputId": "dbfc97a9-75e6-4b6e-e191-3936c3d4a67f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Predictions: tensor([[0.4161],\n",
            "        [0.5759],\n",
            "        [0.4767],\n",
            "        [0.4968],\n",
            "        [0.5347]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations regarding New Predictions:**\n",
        "\n",
        "These values look reasonable and are now within the expected range for probabilities outputted by a logistic regression model. Here's why:\n",
        "\n",
        "1. Range of Predictions: The values (e.g., 0.4161, 0.5759, 0.4767, etc.) are between 0 and 1, which is the valid range for probabilities produced by the sigmoid function. This indicates that the linear outputs are now appropriately scaled.\n",
        "2. Diversity in Predictions: The predictions are no longer saturated at 0 or 1, meaning the model is producing varying outputs that reflect differences in the input data. This is a sign of a well-behaved model.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "0.5759: Approximately a 57.6% probability of belonging to the positive class.\n",
        "\n",
        "0.4161: Approximately a 41.6% probability of belonging to the positive class.\n",
        "\n",
        "These values suggest the model can distinguish between different cases.\n",
        "\n",
        "**Conclusion:**\n",
        "These values are normal and indicate that the issues with extreme outputs have been resolved. We can proceed with further evaluation or training.\n",
        "\n",
        "The predictions of 1, 1, 1, 1 were problematic because they resulted from saturation of the sigmoid function due to extremely large positive linear outputs."
      ],
      "metadata": {
        "id": "RPoTwu0xS6xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    epsilon = 1e-10 # Small value to avoid problem with log(0)\n",
        "    loss = - (true_labels * torch.log(predicted_probs + epsilon) + (1 - true_labels) * torch.log(1 - predicted_probs + epsilon))\n",
        "    return torch.mean(loss)\n",
        "\n",
        "# Calculate the binary cross-entropy loss using 'predictions' and 'targets_tensor'\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "\n",
        "# Output the calculated loss\n",
        "print(\"Binary Cross-Entropy Loss:\", loss.item())"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2141a9-49ca-42db-a0d3-3ab228951ec4"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross-Entropy Loss: 0.6287156343460083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BCE loss value ranges between 0 (perfect predictions) and ∞ (very poor predictions).\n",
        "\n",
        "**Is 0.6287 reasonable?**\n",
        "\n",
        "For a random model, where model is predicting random probabilities with no meaningful relationship to the labels, the BCE loss will typically hover around log(2) ≈ 0.693. This corresponds to a classifier with 50% accuracy on balanced data.\n",
        "\n",
        "**My loss is 0.6287:**\n",
        "\n",
        "Since 0.6287 < 0.693, the model is performing better than random guessing, which is a good sign, especially if this is during early training."
      ],
      "metadata": {
        "id": "uf0XIla3UdM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform backpropagation\n",
        "loss.backward()\n",
        "\n",
        "# Output gradients for w and b\n",
        "print(\"Gradients for w:\", w.grad)\n",
        "print(\"Gradients for b:\", b.grad)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc58b1d3-99be-4e3a-d76a-48cd31f7a6d6"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients for w: tensor([[ 0.1040],\n",
            "        [-0.3537],\n",
            "        [-0.3807]])\n",
            "Gradients for b: tensor([-0.1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do these values appear?**\n",
        "\n",
        "For example, if w.grad = [[0.1040], [-0.3537], [-0.3807]]:\n",
        "\n",
        "0.1040: Increasing w1 slightly would increase the loss by 0.1040.\n",
        "\n",
        "−0.3537: Increasing w2 slightly would decrease the loss by 0.3537.\n",
        "\n",
        "−0.3807: Increasing w3 slightly would decrease the loss by 0.3807.\n",
        "\n",
        "b.grad = −0.1000 means that increasing b slightly would decrease the loss by 0.1000.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The values of w.grad and b.grad are reasonable and indicate that the model is learning. They depend on the input data, loss function, predictions, and initialization."
      ],
      "metadata": {
        "id": "EJFWpobpWuf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = torch.randn(3, 1, requires_grad=True)  # Correct shape for weights\n",
        "b = torch.randn(1, requires_grad=True)    # Bias as scalar\n",
        "\n",
        "# Scale weights and bias in-place, preserving leaf tensors\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000\n",
        "\n",
        "# Compute predictions using the logistic regression model\n",
        "predictions  = model(inputs_tensor, w, b)\n",
        "\n",
        "# Compute the binary cross-entropy loss\n",
        "loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "\n",
        "# Output the calculated loss\n",
        "print(\"Binary Cross-Entropy Loss:\", loss.item())\n",
        "\n",
        "# Perform backpropagation to calculate the gradients\n",
        "loss.backward()\n",
        "\n",
        "# Output the results\n",
        "print(\"Weights after training (w):\", w)\n",
        "print(\"Bias after training (b):\", b)\n",
        "print(\"Gradients for w:\", w.grad)\n",
        "print(\"Gradients for b:\", b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTZTxGNBZkBR",
        "outputId": "9ec85a32-c0fe-46ba-f3cc-4e1983a49f24"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross-Entropy Loss: 0.6930243372917175\n",
            "Weights after training (w): tensor([[6.6135e-04],\n",
            "        [2.6692e-04],\n",
            "        [6.1677e-05]], requires_grad=True)\n",
            "Bias after training (b): tensor([0.0006], requires_grad=True)\n",
            "Gradients for w: tensor([[ 0.0921],\n",
            "        [-0.3602],\n",
            "        [-0.4122]])\n",
            "Gradients for b: tensor([-0.0998])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and conclusion:**\n",
        "1. Loss: The binary cross-entropy loss is 0.693, indicating the model is at a random initialization stage.\n",
        "2. Weights: The weights are small (6.6135e-04), reflecting initial, untuned values.\n",
        "3. Bias: The bias (0.0006) has minimal impact at this stage.\n",
        "4. Gradients: The gradients suggest the second and third features have the most influence on reducing the loss.\n",
        "5. Next Step: Use an optimizer to update the weights and bias, reducing the loss over epochs."
      ],
      "metadata": {
        "id": "xXPQ1-SsaQZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "learning_rate = 0.0001 # Learning rate\n",
        "num_epochs = 1000     # Number of epochs\n",
        "\n",
        "\n",
        "# Gradient descent training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # 1. Generate predictions\n",
        "    predictions = model(inputs_tensor, w, b)\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    loss = binary_cross_entropy(predictions, targets_tensor)\n",
        "\n",
        "    # 3. Compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 4. Update weights (w and b) using the gradients\n",
        "    with torch.no_grad():  # Disable gradient tracking for weight updates\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # 5. Reset the gradients to zero\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "mObHPyE06qsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a482342a-26c1-4941-c44f-71e5a161f44a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Loss: 0.6898547410964966\n",
            "Epoch 200/1000, Loss: 0.6867081522941589\n",
            "Epoch 300/1000, Loss: 0.6835845708847046\n",
            "Epoch 400/1000, Loss: 0.6804836392402649\n",
            "Epoch 500/1000, Loss: 0.6774051189422607\n",
            "Epoch 600/1000, Loss: 0.6743490099906921\n",
            "Epoch 700/1000, Loss: 0.67131507396698\n",
            "Epoch 800/1000, Loss: 0.6683031320571899\n",
            "Epoch 900/1000, Loss: 0.6653130054473877\n",
            "Epoch 1000/1000, Loss: 0.6623445749282837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final predictions after 1000 epochs\n",
        "final_predictions = model(inputs_tensor, w, b)\n",
        "print(\"Final predictions after 1000 epochs:\", final_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kun9hSi5a45R",
        "outputId": "6cb4c5cd-0240-4a5f-8279-7b9feeb67cfe"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final predictions after 1000 epochs: tensor([[0.4912],\n",
            "        [0.5092],\n",
            "        [0.5175],\n",
            "        [0.4761],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations and conclusion:**\n",
        "\n",
        "1. Loss Trend: The binary cross-entropy loss steadily decreased from  0.689 at epoch 100 to 0.662 at epoch 1000, indicating gradual improvement in the model's predictions.\n",
        "2. Convergence: The loss reduction slowed significantly after a few hundred epochs, suggesting the model is approaching convergence but not yet fully optimized.\n",
        "3. Final Predictions: The predictions after 1000 epochs are close to 0.5 for all samples, indicating the model struggles to confidently distinguish between the two classes.\n",
        "4. Model Performance: The lack of confident predictions (≈0.5) suggests that the data or features may not provide sufficient separability for effective classification.\n",
        "5. Next Steps: Consider adding more informative features, adjusting hyperparameters (e.g., learning rate), or regularizing to further improve the model's ability to reduce the loss and make confident predictions."
      ],
      "metadata": {
        "id": "d7zmwRktbN8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KuYKU3OBblIK"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "inputs_tensor = torch.from_numpy(inputs)\n",
        "targets_tensor = torch.from_numpy(targets)\n",
        "\n",
        "# Create TensorDataset\n",
        "dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
        "\n",
        "# Print the first three elements of the dataset\n",
        "for i in range(3):\n",
        "    print(f\"Input {i + 1}: {dataset[i][0]}\")\n",
        "    print(f\"Target {i + 1}: {dataset[i][1]}\")"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a193a0-6bd6-4109-8417-c74909c2c4e0"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 1: tensor([73., 67., 43.])\n",
            "Target 1: tensor([0.])\n",
            "Input 2: tensor([91., 88., 64.])\n",
            "Target 2: tensor([1.])\n",
            "Input 3: tensor([ 87., 134.,  58.])\n",
            "Target 3: tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader with batch size 5 and shuffle enabled\n",
        "train_ds = DataLoader(dataset, batch_size=5, shuffle=True)\n",
        "\n",
        "# Get the first batch from the DataLoader\n",
        "for batch in train_ds:\n",
        "    inputs_batch, targets_batch = batch\n",
        "    print(\"First batch inputs:\\n\", inputs_batch)\n",
        "    print(\"First batch targets:\\n\", targets_batch)\n",
        "    break  # We only need to display the first batch"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047eabe4-c524-4b9b-e82c-efb40ddf44a4"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First batch inputs:\n",
            " tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.],\n",
            "        [102.,  43.,  37.]])\n",
            "First batch targets:\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LogReg class for logistic regression\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LogReg, self).__init__()\n",
        "        # Define a linear layer with input_size input features and 1 output feature\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "        # Sigmoid activation function for logistic regression\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Forward pass through the network\n",
        "    def forward(self, x):\n",
        "        # Apply the linear layer\n",
        "        x = self.linear(x)\n",
        "        # Apply sigmoid activation function\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "In the lecture example, a two-layer network with two linear layers and activation between them is used. For logistic regression, we use only one linear layer for simplicity. We add the Sigmoid activation function to convert the linear combination into a probability (0 to 1 range). The forward pass applies the linear layer to the input data, followed by the activation function to get the result.\n",
        "\n",
        "This sets up and displays the architecture of the logistic regression model (LogReg) defined earlier.\n"
      ],
      "metadata": {
        "id": "BFy367ZmdIBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LogReg model\n",
        "input_size = 3  # We have 3 input features (temp, rainfall, humidity)\n",
        "model = LogReg(input_size)\n",
        "\n",
        "# Display the model architecture\n",
        "print(model)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_QeplfDc87-",
        "outputId": "fbbf8919-1f24-4328-fd2d-a9fb6f4a3e10"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer Stochastic Gradient Descent (SGD)\n",
        "opt = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the binary cross-entropy loss function\n",
        "def binary_cross_entropy_loss(predictions, targets):\n",
        "    return F.binary_cross_entropy(predictions, targets)\n",
        "\n",
        "# Get model predictions\n",
        "predictions = model(inputs_tensor)\n",
        "\n",
        "# Calculate the loss\n",
        "loss = binary_cross_entropy_loss(predictions, targets_tensor)\n",
        "\n",
        "# Output the loss\n",
        "print(f\"Binary Cross-Entropy Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389d5606-cda2-40fd-cdb6-a197671d5f11"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross-Entropy Loss: 7.631152629852295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "Given the high loss value (7.6311), it indicates that the model has not yet learned well."
      ],
      "metadata": {
        "id": "Vr1opaQPdZgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function to track the loss values\n",
        "def fit_return_loss(num_epochs, model, loss, opt, train_ds):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Initialize a loss accumulator\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_ds:\n",
        "            # Generate predictions\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = binary_cross_entropy_loss(pred, yb)\n",
        "\n",
        "            # Perform gradient descent\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Accumulate the loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Print the epoch summary every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 1000 epochs\n",
        "num_epochs = 1000\n",
        "losses = fit_return_loss(num_epochs, model, loss, opt, train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRetruiwdjze",
        "outputId": "17f7a2c6-f46b-4a86-9716-ff1687c86c3b"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 2.1457\n",
            "Epoch [20/1000], Loss: 0.4342\n",
            "Epoch [30/1000], Loss: 0.6794\n",
            "Epoch [40/1000], Loss: 0.3671\n",
            "Epoch [50/1000], Loss: 0.2716\n",
            "Epoch [60/1000], Loss: 0.6356\n",
            "Epoch [70/1000], Loss: 0.3086\n",
            "Epoch [80/1000], Loss: 0.1488\n",
            "Epoch [90/1000], Loss: 0.1773\n",
            "Epoch [100/1000], Loss: 0.1438\n",
            "Epoch [110/1000], Loss: 0.2307\n",
            "Epoch [120/1000], Loss: 0.3256\n",
            "Epoch [130/1000], Loss: 0.1093\n",
            "Epoch [140/1000], Loss: 0.1172\n",
            "Epoch [150/1000], Loss: 0.1540\n",
            "Epoch [160/1000], Loss: 0.0905\n",
            "Epoch [170/1000], Loss: 0.0970\n",
            "Epoch [180/1000], Loss: 0.1315\n",
            "Epoch [190/1000], Loss: 0.2552\n",
            "Epoch [200/1000], Loss: 0.0788\n",
            "Epoch [210/1000], Loss: 0.1109\n",
            "Epoch [220/1000], Loss: 0.0828\n",
            "Epoch [230/1000], Loss: 0.0906\n",
            "Epoch [240/1000], Loss: 0.1015\n",
            "Epoch [250/1000], Loss: 0.0768\n",
            "Epoch [260/1000], Loss: 0.0701\n",
            "Epoch [270/1000], Loss: 0.0784\n",
            "Epoch [280/1000], Loss: 0.0686\n",
            "Epoch [290/1000], Loss: 0.0673\n",
            "Epoch [300/1000], Loss: 0.0587\n",
            "Epoch [310/1000], Loss: 0.0688\n",
            "Epoch [320/1000], Loss: 0.0555\n",
            "Epoch [330/1000], Loss: 0.0591\n",
            "Epoch [340/1000], Loss: 0.0668\n",
            "Epoch [350/1000], Loss: 0.0583\n",
            "Epoch [360/1000], Loss: 0.0510\n",
            "Epoch [370/1000], Loss: 0.0601\n",
            "Epoch [380/1000], Loss: 0.0568\n",
            "Epoch [390/1000], Loss: 0.0554\n",
            "Epoch [400/1000], Loss: 0.0487\n",
            "Epoch [410/1000], Loss: 0.0440\n",
            "Epoch [420/1000], Loss: 0.0534\n",
            "Epoch [430/1000], Loss: 0.0500\n",
            "Epoch [440/1000], Loss: 0.0438\n",
            "Epoch [450/1000], Loss: 0.0610\n",
            "Epoch [460/1000], Loss: 0.0438\n",
            "Epoch [470/1000], Loss: 0.0441\n",
            "Epoch [480/1000], Loss: 0.0392\n",
            "Epoch [490/1000], Loss: 0.0573\n",
            "Epoch [500/1000], Loss: 0.0391\n",
            "Epoch [510/1000], Loss: 0.0424\n",
            "Epoch [520/1000], Loss: 0.0357\n",
            "Epoch [530/1000], Loss: 0.0413\n",
            "Epoch [540/1000], Loss: 0.0412\n",
            "Epoch [550/1000], Loss: 0.0358\n",
            "Epoch [560/1000], Loss: 0.0377\n",
            "Epoch [570/1000], Loss: 0.0451\n",
            "Epoch [580/1000], Loss: 0.0341\n",
            "Epoch [590/1000], Loss: 0.0352\n",
            "Epoch [600/1000], Loss: 0.0400\n",
            "Epoch [610/1000], Loss: 0.0341\n",
            "Epoch [620/1000], Loss: 0.0318\n",
            "Epoch [630/1000], Loss: 0.0323\n",
            "Epoch [640/1000], Loss: 0.0344\n",
            "Epoch [650/1000], Loss: 0.0305\n",
            "Epoch [660/1000], Loss: 0.0331\n",
            "Epoch [670/1000], Loss: 0.0301\n",
            "Epoch [680/1000], Loss: 0.0284\n",
            "Epoch [690/1000], Loss: 0.0295\n",
            "Epoch [700/1000], Loss: 0.0289\n",
            "Epoch [710/1000], Loss: 0.0273\n",
            "Epoch [720/1000], Loss: 0.0269\n",
            "Epoch [730/1000], Loss: 0.0277\n",
            "Epoch [740/1000], Loss: 0.0263\n",
            "Epoch [750/1000], Loss: 0.0291\n",
            "Epoch [760/1000], Loss: 0.0298\n",
            "Epoch [770/1000], Loss: 0.0297\n",
            "Epoch [780/1000], Loss: 0.0283\n",
            "Epoch [790/1000], Loss: 0.0256\n",
            "Epoch [800/1000], Loss: 0.0263\n",
            "Epoch [810/1000], Loss: 0.0243\n",
            "Epoch [820/1000], Loss: 0.0265\n",
            "Epoch [830/1000], Loss: 0.0246\n",
            "Epoch [840/1000], Loss: 0.0240\n",
            "Epoch [850/1000], Loss: 0.0240\n",
            "Epoch [860/1000], Loss: 0.0245\n",
            "Epoch [870/1000], Loss: 0.0232\n",
            "Epoch [880/1000], Loss: 0.0225\n",
            "Epoch [890/1000], Loss: 0.0228\n",
            "Epoch [900/1000], Loss: 0.0227\n",
            "Epoch [910/1000], Loss: 0.0240\n",
            "Epoch [920/1000], Loss: 0.0243\n",
            "Epoch [930/1000], Loss: 0.0230\n",
            "Epoch [940/1000], Loss: 0.0213\n",
            "Epoch [950/1000], Loss: 0.0216\n",
            "Epoch [960/1000], Loss: 0.0214\n",
            "Epoch [970/1000], Loss: 0.0226\n",
            "Epoch [980/1000], Loss: 0.0214\n",
            "Epoch [990/1000], Loss: 0.0208\n",
            "Epoch [1000/1000], Loss: 0.0208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss over epochs\n",
        "plt.plot(range(1, num_epochs + 1), losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "NLWQpcSFdnZw",
        "outputId": "c13f30d4-9805-4ead-96dc-18e41576eaf0"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "Initially, the loss value was quite high (around 20), but it significantly decreased after the first few epochs and stabilized below 0.03 after approximately 200 epochs. This indicates that the model has successfully learned and minimized the error during training.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The loss plot shows a sharp decline at the start of the training and a gradual stabilization, which is typical for models that effectively learn to find patterns in the data.\n"
      ],
      "metadata": {
        "id": "IPCXtudWdt0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final predictions\n",
        "with torch.no_grad():\n",
        "    final_predictions = model(inputs_tensor)\n",
        "    print(\"Final predictions:\\n\", final_predictions)\n",
        "\n",
        "# Print the target values\n",
        "print(\"Target values:\\n\", targets_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARs6W2uyd2HY",
        "outputId": "16e65a8c-e347-47e2-d82e-aedfb0cc4401"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final predictions:\n",
            " tensor([[5.7505e-02],\n",
            "        [9.6006e-01],\n",
            "        [9.9962e-01],\n",
            "        [3.4785e-11],\n",
            "        [1.0000e+00],\n",
            "        [5.7505e-02],\n",
            "        [9.6006e-01],\n",
            "        [9.9962e-01],\n",
            "        [3.4785e-11],\n",
            "        [1.0000e+00],\n",
            "        [5.7505e-02],\n",
            "        [9.6006e-01],\n",
            "        [9.9962e-01],\n",
            "        [3.4785e-11],\n",
            "        [1.0000e+00]])\n",
            "Target values:\n",
            " tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "The final predictions of the model are mostly very close to the target values.\n",
        "\n",
        "For label 0, the prediction is around 0.0575, close to 0.\n",
        "\n",
        "For label 1, the prediction is approximately 0.9996, very close to 1.\n",
        "\n",
        "This shows that the model has learned well to distinguish between the two classes (0 and 1).\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The model successfully reduced the loss during training, with final predictions closely matching the target values. This indicates effective training and accurate predictions, as reflected in the loss graph and final values.\n"
      ],
      "metadata": {
        "id": "Bikv4Xo2d-Or"
      }
    }
  ]
}
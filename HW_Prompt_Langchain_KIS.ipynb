{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrea212023/ML-Kis/blob/main/HW_Prompt_Langchain_KIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Завдання 1: Виклик LLM з базовим промптом\n",
        "\n",
        "Створіть можливість викликати LLM зі звичайним текстовим промптом.\n",
        "\n",
        "Промпт має дозвляти отримати інформацію простою мовою на певну тему. В цьому завданні ми хочемо дізнатись про тему \"Квантові обчислення\".\n",
        "\n",
        "Відповідь моделі повинна містити визначення, ключові переваги та поточні дослідження в цій галузі.\n",
        "\n",
        "Обмежте відповідь до 200 символів і пропишіть в промпті аби відповідь була короткою (це зекономить Вам час і гроші на згенеровані токени).\n",
        "\n",
        "В якості LLM можна скористатись як моделлю з HugginFace (рекомендую Mistral), так і ChatGPT4 або ChatGPT3. В обох випадках треба імпортувати потрібну \"обгортку\" (тобто клас, який дозволить ініціювати модель) з LangChain для виклику LLM за API, а також зчитати особистий токен з файла, наприклад, `creds.json`, який розміщений у Вас локально і Ви НЕ здаєте його в ДЗ і НЕ комітите в git 😏\n",
        "\n",
        "Встановіть своє значення температури на свій розсуд (тут немає правильного чи неправильного значення) і напишіть, чому ви обрали саме таке значення для цього завдання.  \n",
        "\n",
        "Запити можна робити як українською, так і англійською - орієнтуйтесь на те, де і чи хочете ви потім лишити цей проєкт і відповідна яка мова буде пасувати більше. В розвʼязках промпти - українською."
      ],
      "metadata": {
        "id": "RMalK_oYR-X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "id": "vT1ICe74cAUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc5016c-21dd-44ee-ce1e-d77c0fdfa369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain_community huggingface_hub openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DjLUT2LnPgn",
        "outputId": "41c68e6f-79de-48a0-b4d8-ade7b41945cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mkzNqsenT8_",
        "outputId": "9e443f5c-6690-4ffd-f47b-8381a5ac85c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "g8vUm-Qkn_hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creds = '/content/drive/MyDrive/Colab Notebooks/data/creds.json'\n",
        "\n",
        "# Read creds.json\n",
        "with open(creds, 'r') as f:\n",
        "    creds = json.load(f)\n",
        "\n",
        "api_key = creds[\"openai_token\"]\n"
      ],
      "metadata": {
        "id": "4k3FEaddoEak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Initialize the model using the API key with a temperature of 0.3 for less randomness\n",
        "llm = OpenAI(api_key=api_key, temperature=0.3)\n",
        "\n",
        "# Example prompt asking for a brief answer on the topic of quantum computing\n",
        "prompt = \"\"\"\n",
        "Квантові обчислення: визначення, ключові переваги та поточні дослідження в галузі.\n",
        "Коротко і до 200 символів, будь ласка.\n",
        "\"\"\"\n",
        "\n",
        "# Implement retry mechanism with exponential backoff\n",
        "max_retries = 5\n",
        "retry_delay = 1  # Initial delay in seconds\n",
        "\n",
        "for attempt in range(max_retries):\n",
        "    try:\n",
        "        # Get the model's response\n",
        "        response = llm(prompt)\n",
        "        # Print the response\n",
        "        print(response)\n",
        "        break  # Exit the loop if successful\n",
        "    except Exception as e:\n",
        "        if \"429\" in str(e):\n",
        "            print(f\"Rate limit exceeded (attempt {attempt + 1}/{max_retries}). Retrying in {retry_delay} seconds...\")\n",
        "            time.sleep(retry_delay)\n",
        "            retry_delay *= 2  # Exponential backoff\n",
        "        else:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break  # Exit the loop for other errors\n",
        "else:\n",
        "    print(\"Failed to get a response after multiple retries.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gp3BnDyrfTY",
        "outputId": "b3078d3a-11f9-44cc-d900-49aae6abc040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Квантові обчислення - це обчислювальна технологія, яка використовує квантові біти замість класичних бітів для зберігання та обробки інформації. Вони мають потенціал вирішувати складні задачі швидше, ефективніше та точніше, ніж традиційні комп'ютери. Ключові переваги квантових обчислень - це висока швидкість та потужність обчислень, здатність до розв'язання складних задач, які недосяжні для класичних комп'ютерів, та можливість використання в галузях\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation for temperature:**\n",
        "\n",
        "I choose a temperature setting of 0.3 because it strikes a balance between creativity and accuracy. For tasks requiring concise and informative responses that stay on topic, a low temperature setting (like 0.3) helps minimize random variations in the answers, while maintaining clarity and structure."
      ],
      "metadata": {
        "id": "KtqtgMzYtD-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання 2: Створення параметризованого промпта для генерації тексту\n",
        "Тепер ми хочемо оновити попередній фукнціонал так, аби в промпт ми могли передавати тему як параметр. Для цього скористайтесь `PromptTemplate` з `langchain` і реалізуйте параметризований промпт та виклик моделі з ним.\n",
        "\n",
        "Запустіть оновлений функціонал (промпт + модел) для пояснень про теми\n",
        "- \"Баєсівські методи в машинному навчанні\"\n",
        "- \"Трансформери в машинному навчанні\"\n",
        "- \"Explainable AI\"\n",
        "\n",
        "Виведіть результати відпрацювання моделі на екран."
      ],
      "metadata": {
        "id": "UiIzV0UIS0GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Prompt Template\n",
        "template = \"\"\"\n",
        "Поясніть тему \"{topic}\" простими словами. Будь ласка, дайте коротке пояснення.\n",
        "\"\"\"\n",
        "\n",
        "# Create a PromptTemplate object with the \"topic\" parameter\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Topics to explain\n",
        "topics = [\n",
        "   \"Баєсівські методи в машинному навчанні\",\n",
        "\"Трансформери в машинному навчанні\",\n",
        "\"Explainable AI\",\n",
        "]\n",
        "\n",
        "# Loop to generate explanations for each topic\n",
        "for topic in topics:\n",
        "    # Generate prompt using parameter\n",
        "    prompt = prompt_template.format(topic=topic)\n",
        "\n",
        "    # Getting response from model\n",
        "    response = llm(prompt)\n",
        "\n",
        "    # Output the result\n",
        "    print(f\"Topic: {topic}\\nAnswer: {response}\\n\")"
      ],
      "metadata": {
        "id": "kcg7EfdNcBDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ace4fd-e5e5-4dbe-f8f8-7dd665ee0ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: Баєсівські методи в машинному навчанні\n",
            "Answer: \n",
            "Баєсівські методи в машинному навчанні - це підхід до розв'язання задач штучного інтелекту, який базується на теорії ймовірності і використовує статистичні методи для прийняття рішень. Вони дозволяють моделювати невизначеність та непевність в даних та враховувати нову інформацію для поліпшення результатів. Ці методи широко використовуються в різних областях, наприклад, для розпізнавання образів, класифікації даних, прогнозування та рекомендацій. \n",
            "\n",
            "Topic: Трансформери в машинному навчанні\n",
            "Answer: \n",
            "Трансформери в машинному навчанні - це алгоритми, які використовуються для обробки текстової інформації. Вони використовуються для перетворення слів у числа, що дозволяє комп'ютеру зрозуміти текст і виконувати завдання, пов'язані з мовою, наприклад, переклад або розпізнавання мови. Трансформери допомагають комп'ютеру \"вчитися\" мові, аналізуючи великі обсяги тексту і виявляючи закономірності в ньому. Вони є важливим інструментом для розвитку штучного інтелекту і допомага\n",
            "\n",
            "Topic: Explainable AI\n",
            "Answer: \n",
            "Explainable AI - це підхід до розробки штучного інтелекту, який дозволяє зрозуміти причини прийнятих ним рішень. Це означає, що система штучного інтелекту повинна пояснювати свої дії та логіку своїх рішень людям, щоб вони могли перевірити, чи вони є обґрунтованими та правильними. Це дозволяє збільшити довіру до системи та зробити її роботу більш зрозумілою та прозорою для користувачів. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Завдання 3: Використання агента для автоматизації процесів\n",
        "Створіть агента, який допоможе автоматично шукати інформацію про останні наукові публікації в різних галузях. Наприклад, агент має знайти 5 останніх публікацій на тему штучного інтелекту.\n",
        "\n",
        "**Кроки:**\n",
        "1. Налаштуйте агента типу ReAct в LangChain для виконання автоматичних запитів.\n",
        "2. Створіть промпт, який спрямовує агента шукати інформацію в інтернеті або в базах даних наукових публікацій.\n",
        "3. Агент повинен видати список публікацій, кожна з яких містить назву, авторів і короткий опис.\n",
        "\n",
        "Для взаємодії з пошуком там необхідно створити `Tool`. В лекції ми використовували `serpapi`. Можна продовжити користуватись ним, або обрати інше АРІ для пошуку (вони в тому числі є безкоштовні). Перелік різних АРІ, доступних в langchain, і орієнтир по вартості запитів можна знайти в окремому документі [тут](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
        "\n",
        "Лишаю також нижче приклад використання одного з безкоштовних пошукових АРІ - DuckDuckGo (не потребує створення токена!)  - можливо він вам сподобається :)\n"
      ],
      "metadata": {
        "id": "m9UsL2gXSe-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community duckduckgo_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzj4PHP9XwH-",
        "outputId": "f656403d-6c77-4cad-adc8-3f5b084febd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/3.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/3.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "search.invoke(\"Obama's first name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "p56AZ_SnXvTs",
        "outputId": "7917ad7a-05ab-48df-8a51-3c6f7bd2a57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The first president, ... Barack Obama (b. 1961) ... President-elect. List of presidents of the United States from 1789 - till date. No. Portrait Name (Birth-Death) Term Party Election Vice President-elect 47: Donald Trump (b. 1946) [77] To be sworn in on January 20, 2025 Republican: 2024: 2 of 2. Barack Obama: timeline Key events in the life of Barack Obama. Barack Obama (born August 4, 1961, Honolulu, Hawaii, U.S.) is the 44th president of the United States (2009-17) and the first African American to hold the office. Before winning the presidency, Obama represented Illinois in the U.S. Senate (2005-08). Here is a list of the presidents and vice presidents of the United States along with their parties and dates in office. ... Chester A Arthur: Twenty-First President of the United States. 10 Interesting Facts About James Buchanan. Martin Van Buren - Eighth President of the United States. Quotes From Harry S. Truman. Michelle LaVaughn Robinson Obama [1] (née Robinson; born January 17, 1964) is an American attorney and author who served as the first lady of the United States from 2009 to 2017, being married to Barack Obama, the 44th president of the United States.. Raised on the South Side of Chicago, Obama is a graduate of Princeton University and Harvard Law School. Barack Hussein Obama II (US i / b ə ˈ r ɑː k h uː ˈ s eɪ n ɵ ˈ b ɑː m ə /, born August 4, 1961) is the 44th and current President of the United States, and the first African American to hold the office. Born in Honolulu, Hawaii, Obama is a graduate of Columbia University and Harvard Law School, where he served as president of the Harvard Law Review.He was a community organizer in ...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization requested by DuckDuckGo\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Performing a search for publications on the topic of \"artificial intelligence\"\n",
        "query = \"latest groundbreaking scientific publications on artificial intelligence\"\n",
        "results = search.invoke(query)\n",
        "\n",
        "# Output the result by lines\n",
        "formatted_results = results.split('. ') # Break the text into sentences\n",
        "\n",
        "# Display each sentence separately\n",
        "for sentence in formatted_results:\n",
        "    print(sentence.strip() + \".\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPhOLpd5tznE",
        "outputId": "9fb902f4-5c32-4da9-f5c4-bf0683b6806e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distributional Graphormer (DIG) is a deep learning framework for predicting protein structures with greater accuracy, a fundamental problem in molecular science.\n",
            "\n",
            "This advance could help deliver breakthroughs in critical research areas like materials science and drug discovery.\n",
            "\n",
            "The proliferation of artificial intelligence tools in scientific research risks creating illusions of understanding, where&nbsp;scientists believe they understand more about the world than they ...\n",
            "\n",
            "Unfortunately, other discussions, such as those on artificial general intelligence (AGI) and 'superintelligence', are based on an understanding of AI that is more rooted in science fiction ...\n",
            "\n",
            "AI is rapidly advancing science, with breakthroughs in fields like protein structure prediction, brain mapping, and flood forecasting.\n",
            "\n",
            "These advancements are built on collaborations between researchers, technologists, and policymakers, and they offer a blueprint for how AI can improve human life.\n",
            "\n",
            "The rapid advances in artificial intelligence (AI) may lead to massive value creation and capture across many facets of human society 1,2,3,4,5, creating a wealth of social and economic ....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improvement of query:**\n",
        "\n",
        "Let's look for our request 'artificial intelligence' in semantic scholar."
      ],
      "metadata": {
        "id": "8F6i7QQovEVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Function for performing a search for publications\n",
        "def search_publications(query, limit=5):\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"fields\": \"title,authors,year,url\",\n",
        "        \"limit\": limit\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get(\"data\", [])\n",
        "    else:\n",
        "        print(\"Error:\", response.status_code)\n",
        "        return []\n",
        "\n",
        "# Search publications on the topic \"artificial intelligence\"\n",
        "query = \"artificial intelligence\"\n",
        "publications = search_publications(query)\n",
        "\n",
        "# Output results: Title, authors, year and link\n",
        "for pub in publications:\n",
        "    title = pub.get('title', 'No title')\n",
        "    authors = ', '.join([author['name'] for author in pub.get('authors', [])])\n",
        "    year = pub.get('year', 'No year')\n",
        "    url = pub.get('url', 'No URL')\n",
        "    print(f\"Title: {title}\\nAuthors: {authors}\\nYear: {year}\\nURL: {url}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hguyyStZuYIA",
        "outputId": "da2bdfc7-3998-4d4f-bb4a-5b284fcdf3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI\n",
            "Authors: Alejandro Barredo Arrieta, Natalia Díaz Rodríguez, J. Ser, Adrien Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-Lopez, D. Molina, Richard Benjamins, Raja Chatila, Francisco Herrera\n",
            "Year: 2019\n",
            "URL: https://www.semanticscholar.org/paper/530a059cb48477ad1e3d4f8f4b153274c8997332\n",
            "\n",
            "Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
            "Authors: Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, J. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Y. Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
            "Year: 2023\n",
            "URL: https://www.semanticscholar.org/paper/8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c\n",
            "\n",
            "Title: High-performance medicine: the convergence of human and artificial intelligence\n",
            "Authors: E. Topol\n",
            "Year: 2019\n",
            "URL: https://www.semanticscholar.org/paper/f134abeaf9bfd41f29b97aec675ec31895bf541d\n",
            "\n",
            "Title: Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)\n",
            "Authors: Amina Adadi, M. Berrada\n",
            "Year: 2018\n",
            "URL: https://www.semanticscholar.org/paper/21dff47a4142445f83016da0819ffe6dd2947f66\n",
            "\n",
            "Title: ARTIFICIAL INTELLIGENCE FOR THE REAL WORLD\n",
            "Authors: \n",
            "Year: 2023\n",
            "URL: https://www.semanticscholar.org/paper/7b72711ac2ea7bd7f519cac162a4a6578bbb7d0d\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improvement of query:**\n",
        "\n",
        "Articles output by semantic scholar is too old.\n",
        "Let's look for our request 'artificial intelligence' in arXiv."
      ],
      "metadata": {
        "id": "Lrc3WP_BvSDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def search_publications_arxiv(query, max_results=5):\n",
        "    \"\"\"\n",
        "    Searches arXiv for publications based on a given query.\n",
        "\n",
        "    Args:\n",
        "        query: The search query string.\n",
        "        max_results: The maximum number of results to return.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary represents a publication\n",
        "        with 'title', 'authors', 'published', and 'summary' keys.\n",
        "    \"\"\"\n",
        "    base_url = \"http://export.arxiv.org/api/query\"\n",
        "    params = {\n",
        "        \"search_query\": query,\n",
        "        \"max_results\": max_results,\n",
        "        \"sortBy\": \"lastUpdatedDate\",  # Sort by last updated date\n",
        "        \"sortOrder\": \"descending\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    # Parse the XML response\n",
        "    import xml.etree.ElementTree as ET\n",
        "    root = ET.fromstring(response.content)\n",
        "    entries = root.findall(\"{http://www.w3.org/2005/Atom}entry\")\n",
        "\n",
        "    publications = []\n",
        "    for entry in entries:\n",
        "        title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
        "        authors = \", \".join([author.text for author in entry.findall(\"{http://www.w3.org/2005/Atom}author/{http://www.w3.org/2005/Atom}name\")])\n",
        "        published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
        "        summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
        "        publications.append({\n",
        "            \"title\": title,\n",
        "            \"authors\": authors,\n",
        "            \"published\": published,\n",
        "            \"summary\": summary\n",
        "        })\n",
        "    return publications\n",
        "\n",
        "\n",
        "# Example usage\n",
        "query = \"artificial intelligence\"\n",
        "arxiv_publications = search_publications_arxiv(query)\n",
        "\n",
        "# Print the results\n",
        "for pub in arxiv_publications:\n",
        "    print(f\"Title: {pub['title']}\\nAuthors: {pub['authors']}\\nPublished: {pub['published']}\\nSummary: {pub['summary']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRbezxhCu2VU",
        "outputId": "f0e95862-0eab-4367-d283-6f9354843055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n",
            "  Models\n",
            "Authors: Salma Kharrat, Fares Fourati, Marco Canini\n",
            "Published: 2024-11-19T18:58:03Z\n",
            "Summary:   The effectiveness of Large Language Models (LLMs) in solving tasks vastly\n",
            "depends on the quality of the instructions, which often require fine-tuning\n",
            "through extensive human effort. This highlights the need for automated\n",
            "instruction optimization; however, this optimization is particularly\n",
            "challenging when dealing with black-box LLMs, where model parameters and\n",
            "gradients remain inaccessible. We propose ACING, a task-specific prompt\n",
            "optimization approach framed as a stateless continuous-action Reinforcement\n",
            "Learning (RL) problem, known as the continuum bandit setting. ACING leverages\n",
            "an actor-critic-based method to optimize prompts, learning from\n",
            "non-differentiable reward signals. We validate ACING by optimizing prompts for\n",
            "ChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\n",
            "methods, achieving a median score improvement of 10 percentage points.\n",
            "Furthermore, ACING not only recovers but also surpasses human-crafted expert\n",
            "instructions, achieving up to a 39 percentage point improvement against human\n",
            "benchmarks.\n",
            "\n",
            "\n",
            "Title: Benchmarking Positional Encodings for GNNs and Graph Transformers\n",
            "Authors: Florian Grötschla, Jiaqing Xie, Roger Wattenhofer\n",
            "Published: 2024-11-19T18:57:01Z\n",
            "Summary:   Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs)\n",
            "have been driven by innovations in architectures and Positional Encodings\n",
            "(PEs), which are critical for augmenting node features and capturing graph\n",
            "topology. PEs are essential for GTs, where topological information would\n",
            "otherwise be lost without message-passing. However, PEs are often tested\n",
            "alongside novel architectures, making it difficult to isolate their effect on\n",
            "established models. To address this, we present a comprehensive benchmark of\n",
            "PEs in a unified framework that includes both message-passing GNNs and GTs. We\n",
            "also establish theoretical connections between MPNNs and GTs and introduce a\n",
            "sparsified GRIT attention mechanism to examine the influence of global\n",
            "connectivity. Our findings demonstrate that previously untested combinations of\n",
            "GNN architectures and PEs can outperform existing methods and offer a more\n",
            "comprehensive picture of the state-of-the-art. To support future research and\n",
            "experimentation in our framework, we make the code publicly available.\n",
            "\n",
            "\n",
            "Title: Heuristic-Free Multi-Teacher Learning\n",
            "Authors: Huy Thong Nguyen, En-Hung Chu, Lenord Melvix, Jazon Jiao, Chunglin Wen, Benjamin Louie\n",
            "Published: 2024-11-19T18:45:16Z\n",
            "Summary:   We introduce Teacher2Task, a novel framework for multi-teacher learning that\n",
            "eliminates the need for manual aggregation heuristics. Existing multi-teacher\n",
            "methods typically rely on such heuristics to combine predictions from multiple\n",
            "teachers, often resulting in sub-optimal aggregated labels and the propagation\n",
            "of aggregation errors. Teacher2Task addresses these limitations by introducing\n",
            "teacher-specific input tokens and reformulating the training process. Instead\n",
            "of relying on aggregated labels, the framework transforms the training data,\n",
            "consisting of ground truth labels and annotations from N teachers, into N+1\n",
            "distinct tasks: N auxiliary tasks that predict the labeling styles of the N\n",
            "individual teachers, and one primary task that focuses on the ground truth\n",
            "labels. This approach, drawing upon principles from multiple learning\n",
            "paradigms, demonstrates strong empirical results across a range of\n",
            "architectures, modalities, and tasks.\n",
            "\n",
            "\n",
            "Title: An AI-Enabled Side Channel Power Analysis Based Hardware Trojan\n",
            "  Detection Method for Securing the Integrated Circuits in Cyber-Physical\n",
            "  Systems\n",
            "Authors: Sefatun-Noor Puspa, Abyad Enan, Reek Majumdar, M Sabbir Salek, Gurcan Comert, Mashrur Chowdhury\n",
            "Published: 2024-11-19T18:39:20Z\n",
            "Summary:   Cyber-physical systems rely on sensors, communication, and computing, all\n",
            "powered by integrated circuits (ICs). ICs are largely susceptible to various\n",
            "hardware attacks with malicious intents. One of the stealthiest threats is the\n",
            "insertion of a hardware trojan into the IC, causing the circuit to malfunction\n",
            "or leak sensitive information. Due to supply chain vulnerabilities, ICs face\n",
            "risks of trojan insertion during various design and fabrication stages. These\n",
            "trojans typically remain inactive until triggered. Once triggered, trojans can\n",
            "severely compromise system safety and security. This paper presents a\n",
            "non-invasive method for hardware trojan detection based on side-channel power\n",
            "analysis. We utilize the dynamic power measurements for twelve hardware trojans\n",
            "from IEEE DataPort. Our approach applies to signal processing techniques to\n",
            "extract crucial time-domain and frequency-domain features from the power\n",
            "traces, which are then used for trojan detection leveraging Artificial\n",
            "Intelligence (AI) models. Comparison with a baseline detection approach\n",
            "indicates that our approach achieves higher detection accuracy than the\n",
            "baseline models used on the same side-channel power dataset.\n",
            "\n",
            "\n",
            "Title: CATCH: Complementary Adaptive Token-level Contrastive Decoding to\n",
            "  Mitigate Hallucinations in LVLMs\n",
            "Authors: Zhehan Kan, Ce Zhang, Zihan Liao, Yapeng Tian, Wenming Yang, Junyuan Xiao, Xu Li, Dongmei Jiang, Yaowei Wang, Qingmin Liao\n",
            "Published: 2024-11-19T18:27:31Z\n",
            "Summary:   Large Vision-Language Model (LVLM) systems have demonstrated impressive\n",
            "vision-language reasoning capabilities but suffer from pervasive and severe\n",
            "hallucination issues, posing significant risks in critical domains such as\n",
            "healthcare and autonomous systems. Despite previous efforts to mitigate\n",
            "hallucinations, a persistent issue remains: visual defect from vision-language\n",
            "misalignment, creating a bottleneck in visual processing capacity. To address\n",
            "this challenge, we develop Complementary Adaptive Token-level Contrastive\n",
            "Decoding to Mitigate Hallucinations in LVLMs (CATCH), based on the Information\n",
            "Bottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for\n",
            "visual information separation, Non-Visual Screening (NVS) for hallucination\n",
            "detection, and Adaptive Token-level Contrastive Decoding (ATCD) for\n",
            "hallucination mitigation. CATCH addresses issues related to visual defects that\n",
            "cause diminished fine-grained feature perception and cumulative hallucinations\n",
            "in open-ended scenarios. It is applicable to various visual question-answering\n",
            "tasks without requiring any specific data or prior knowledge, and generalizes\n",
            "robustly to new tasks without additional training, opening new possibilities\n",
            "for advancing LVLM in various challenging applications.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improvement of query:**\n",
        "\n",
        "Let's look for our request 'artificial intelligence' in pubmed."
      ],
      "metadata": {
        "id": "nRE6faDavfK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_publications_pubmed(query, max_results=5):\n",
        "    \"\"\"\n",
        "    Searches PubMed for publications based on a query.\n",
        "\n",
        "    Args:\n",
        "      query: The search query string.\n",
        "      max_results: The maximum number of results to retrieve.\n",
        "\n",
        "    Returns:\n",
        "      A list of dictionaries, where each dictionary represents a publication\n",
        "      with \"title,\" \"authors,\" and \"url\" keys.\n",
        "    \"\"\"\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmax\": max_results,\n",
        "        \"retmode\": \"json\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    data = response.json()\n",
        "    id_list = data[\"esearchresult\"][\"idlist\"]\n",
        "\n",
        "    publications = []\n",
        "    if id_list:\n",
        "        base_url_summary = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
        "        params_summary = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"id\": \",\".join(id_list),\n",
        "            \"retmode\": \"json\"\n",
        "        }\n",
        "        response_summary = requests.get(base_url_summary, params=params_summary)\n",
        "        response_summary.raise_for_status()\n",
        "        summary_data = response_summary.json()\n",
        "        for pubmed_id, details in summary_data['result'].items():\n",
        "            if pubmed_id != 'uids':\n",
        "               publications.append({\n",
        "                  \"title\": details.get(\"title\", \"No title\"),\n",
        "                  \"authors\": details.get(\"authors\", \"No authors\"),\n",
        "                  \"url\": f\"https://pubmed.ncbi.nlm.nih.gov/{pubmed_id}/\"\n",
        "               })\n",
        "    return publications\n",
        "\n",
        "# Example usage:\n",
        "query = \"artificial intelligence\"\n",
        "pubmed_publications = search_publications_pubmed(query)\n",
        "\n",
        "# Print results\n",
        "for pub in pubmed_publications:\n",
        "    print(f\"Title: {pub['title']}\\nAuthors: {pub['authors']}\\nURL: {pub['url']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79dNJB1_vAAH",
        "outputId": "a14f9ba7-d268-4e80-a337-9b6bc0e90b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Creating an Empirical Dermatology Dataset Through Crowdsourcing With Web Search Advertisements.\n",
            "Authors: [{'name': 'Ward A', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Li J', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Wang J', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Lakshminarasimhan S', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Carrick A', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Campana B', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Hartford J', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Sreenivasaiah PK', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Tiyasirisokchai T', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Virmani S', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Wong R', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Matias Y', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Corrado GS', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Webster DR', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Smith MA', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Siegel D', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Lin S', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Ko J', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Karthikesalingam A', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Semturs C', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Rao P', 'authtype': 'Author', 'clusterid': ''}]\n",
            "URL: https://pubmed.ncbi.nlm.nih.gov/39565619/\n",
            "\n",
            "Title: TIRADS-based artificial intelligence systems for ultrasound images of thyroid nodules: protocol for a systematic review.\n",
            "Authors: [{'name': 'Sharifi Y', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Amiri Tehranizadeh A', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Danay Ashgzari M', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Naseri Z', 'authtype': 'Author', 'clusterid': ''}]\n",
            "URL: https://pubmed.ncbi.nlm.nih.gov/39565572/\n",
            "\n",
            "Title: Enhancing detection of high-level axillary lymph node metastasis after neoadjuvant therapy in breast cancer patients with nodal involvement: a combined approach of axilla ultrasound and breast elastography.\n",
            "Authors: [{'name': 'Huang JX', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Liu FT', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Tan YT', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Wang XY', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Huang JH', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Lin SY', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Huang GL', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Zhang YT', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Pei XQ', 'authtype': 'Author', 'clusterid': ''}]\n",
            "URL: https://pubmed.ncbi.nlm.nih.gov/39565571/\n",
            "\n",
            "Title: Wearable EEG Neurofeedback Based-on Machine Learning Algorithms for Children with Autism: A Randomized, Placebo-controlled Study.\n",
            "Authors: [{'name': 'Wang XN', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Zhang T', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Han BC', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Luo WW', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Liu WH', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Yang ZY', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Disi A', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Sun Y', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Yang JC', 'authtype': 'Author', 'clusterid': ''}]\n",
            "URL: https://pubmed.ncbi.nlm.nih.gov/39565505/\n",
            "\n",
            "Title: An AI deep learning algorithm for detecting pulmonary nodules on ultra-low-dose CT in an emergency setting: a reader study.\n",
            "Authors: [{'name': 'van den Berk IAH', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Jacobs C', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Kanglie MMNP', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Mets OM', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Snoeren M', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Montauban van Swijndregt AD', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Taal EM', 'authtype': 'Author', 'clusterid': ''}, {'name': 'van Engelen TSR', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Prins JM', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Bipat S', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Bossuyt PMM', 'authtype': 'Author', 'clusterid': ''}, {'name': 'Stoker J', 'authtype': 'Author', 'clusterid': ''}, {'name': 'OPTIMACT study group', 'authtype': 'CollectiveName', 'clusterid': ''}]\n",
            "URL: https://pubmed.ncbi.nlm.nih.gov/39565453/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Завдання 4: Створення агента-помічника для вирішення бізнес-задач\n",
        "\n",
        "Створіть агента, який допомагає вирішувати задачі бізнес-аналітики. Агент має допомогти користувачу створити прогноз по продажам на наступний рік враховуючи рівень інфляції і погодні умови. Агент має вміти використовувати Python і ходити в інтернет аби отримати актуальні дані.\n",
        "\n",
        "**Кроки:**\n",
        "1. Налаштуйте агента, який працюватиме з аналітичними даними, заданими текстом. Користувач пише\n",
        "\n",
        "```\n",
        "Ми експортуємо апельсини з Бразилії. В 2021 експортували 200т, в 2022 - 190т, в 2023 - 210т, в 2024 який ще не закінчився - 220т. Зроби оцінку скільки ми зможемо експортувати апельсинів в 2025 враховуючи погодні умови в Бразилії і попит на апельсини в світі виходячи з економічної ситуації.\n",
        "```\n",
        "\n",
        "2. Створіть запит до агента, що містить чітке завдання – видати результат бізнес аналізу або написати, що він не може цього зробити і запит користувача (просто може бути все одним повідомлленням).\n",
        "\n",
        "3. Запустіть агента і проаналізуйте результати. Що можна покращити?\n"
      ],
      "metadata": {
        "id": "IOqujC6qY_NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_agent = '/content/drive/MyDrive/Colab Notebooks/data/my_agent.json'\n",
        "\n",
        "# Read creds.json\n",
        "with open(my_agent, 'r') as f:\n",
        "    my_agent = json.load(f)\n",
        "\n",
        "weather_api_key = my_agent[\"weather_api_key\"]\n",
        "inflation_api_key = my_agent[\"inflation_api_key\"]"
      ],
      "metadata": {
        "id": "n0xzcX22717R"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define city\n",
        "city = \"Sao Paulo\"\n",
        "\n",
        "# Construct the WeatherAPI URL\n",
        "url = f\"http://api.weatherapi.com/v1/current.json?key={weather_api_key}&q={city}&aqi=no\"\n",
        "\n",
        "# Send the request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check the response status\n",
        "if response.status_code == 200:\n",
        "    weather_data = response.json()\n",
        "    print(weather_data)  # Debugging output\n",
        "\n",
        "    # Extract weather data\n",
        "    temperature = weather_data.get('current', {}).get('temp_c', 'No data')\n",
        "    precipitation = weather_data.get('current', {}).get('precip_mm', 0)\n",
        "    weather_description = weather_data.get('current', {}).get('condition', {}).get('text', 'No data')\n",
        "\n",
        "    # Format and print the weather info\n",
        "    weather_info = f\"Temperature in {city}: {temperature}°C, precipitation: {precipitation} mm, condition: {weather_description}.\"\n",
        "    print(weather_info)\n",
        "else:\n",
        "    print(f\"Request error: {response.status_code} - {response.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAEacC8hDDZr",
        "outputId": "43c1d063-2448-405f-f9a6-4f684d6331a0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'location': {'name': 'Sao Paulo', 'region': 'Sao Paulo', 'country': 'Brazil', 'lat': -23.5333, 'lon': -46.6167, 'tz_id': 'America/Sao_Paulo', 'localtime_epoch': 1732147302, 'localtime': '2024-11-20 21:01'}, 'current': {'last_updated_epoch': 1732147200, 'last_updated': '2024-11-20 21:00', 'temp_c': 21.2, 'temp_f': 70.2, 'is_day': 0, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/night/116.png', 'code': 1003}, 'wind_mph': 8.9, 'wind_kph': 14.4, 'wind_degree': 159, 'wind_dir': 'SSE', 'pressure_mb': 1016.0, 'pressure_in': 30.0, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 88, 'cloud': 75, 'feelslike_c': 21.2, 'feelslike_f': 70.2, 'windchill_c': 20.8, 'windchill_f': 69.4, 'heatindex_c': 20.8, 'heatindex_f': 69.4, 'dewpoint_c': 17.5, 'dewpoint_f': 63.5, 'vis_km': 6.0, 'vis_miles': 3.0, 'uv': 0.0, 'gust_mph': 10.3, 'gust_kph': 16.6}}\n",
            "Temperature in Sao Paulo: 21.2°C, precipitation: 0.0 mm, condition: Partly cloudy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the country for which you want inflation data\n",
        "country = \"Brazil\"  # Replace with your desired country\n",
        "\n",
        "# Construct the API request URL\n",
        "url_inflation = f\"https://api.api-ninjas.com/v1/inflation?country={country}\"\n",
        "\n",
        "# Set the headers with the API key\n",
        "headers = {\n",
        "    'X-Api-Key': inflation_api_key\n",
        "}\n",
        "\n",
        "# Execute the request\n",
        "response_inflation = requests.get(url_inflation, headers=headers)\n",
        "\n",
        "# Check the response status\n",
        "if response_inflation.status_code == 200:\n",
        "    inflation_data = response_inflation.json()\n",
        "    if inflation_data:\n",
        "        inflation_rate = inflation_data[0]['yearly_rate_pct']\n",
        "        print(f\"The current inflation rate in {country} is {inflation_rate}%.\")\n",
        "    else:\n",
        "        print(f\"No inflation data available for {country}.\")\n",
        "else:\n",
        "    print(f\"Error: {response_inflation.status_code} - {response_inflation.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njkFo9o1BZBb",
        "outputId": "2ff97176-113e-40ec-e1b9-c514eee95d3a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 400 - {\"error\": \"The requested country inflation data is for premium subscribers only.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota bene:**\n",
        "\n",
        "Unfortunately, the API I used provides data for premium subscribers only. Let's try to bypass with rapidapi."
      ],
      "metadata": {
        "id": "2D6oci07Hzwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import http.client\n",
        "\n",
        "conn = http.client.HTTPSConnection(\"inflation-by-api-ninjas.p.rapidapi.com\")\n",
        "\n",
        "headers = {\n",
        "    'x-rapidapi-key': \"063dfdbe76mshe5b144c25dfc7fep11f051jsnd8ecd467b712\",\n",
        "    'x-rapidapi-host': \"inflation-by-api-ninjas.p.rapidapi.com\"\n",
        "}\n",
        "\n",
        "conn.request(\"GET\", \"/v1/inflation\", headers=headers)\n",
        "\n",
        "res = conn.getresponse()\n",
        "data = res.read()\n",
        "\n",
        "print(data.decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ3jwR1sHHfX",
        "outputId": "6e73d6d3-4348-4ac7-bdf6-4f26aa54140f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"country\": \"Austria\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.32, \"yearly_rate_pct\": 1.81}, {\"country\": \"Belgium\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.48, \"yearly_rate_pct\": 3.2}, {\"country\": \"Canada\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.43, \"yearly_rate_pct\": 2.02}, {\"country\": \"Chile\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.98, \"yearly_rate_pct\": 4.72}, {\"country\": \"Estonia\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.6, \"yearly_rate_pct\": 4.08}, {\"country\": \"Germany\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.42, \"yearly_rate_pct\": 2.04}, {\"country\": \"Hungary\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.09, \"yearly_rate_pct\": 3.17}, {\"country\": \"Iceland\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.29, \"yearly_rate_pct\": 5.07}, {\"country\": \"Ireland\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.3, \"yearly_rate_pct\": 0.7}, {\"country\": \"Luxembourg\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": -0.32, \"yearly_rate_pct\": 0.95}, {\"country\": \"Norway\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.6, \"yearly_rate_pct\": 2.59}, {\"country\": \"Portugal\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.06, \"yearly_rate_pct\": 2.32}, {\"country\": \"Russia\", \"type\": \"CPI\", \"period\": \"Mar 2022\", \"monthly_rate_pct\": 7.61, \"yearly_rate_pct\": 16.7}, {\"country\": \"Slovakia\", \"type\": \"CPI\", \"period\": \"Sep 2024\", \"monthly_rate_pct\": 0.04, \"yearly_rate_pct\": 2.58}, {\"country\": \"Sweden\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.23, \"yearly_rate_pct\": 1.57}, {\"country\": \"Switzerland\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": -0.13, \"yearly_rate_pct\": 0.62}, {\"country\": \"The Netherlands\", \"type\": \"CPI\", \"period\": \"Oct 2024\", \"monthly_rate_pct\": 0.49, \"yearly_rate_pct\": 3.55}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(api_key=api_key, temperature=0.3)\n",
        "\n",
        "prompt = \"\"\"\n",
        "Define inflation in Brazil during 2024. Give montly rate for November and yearly rate for 2024. Limit your response to 50 words.\n",
        "\"\"\"\n",
        "\n",
        "inflation_rate = llm(prompt)\n",
        "inflation_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "XM22RN-CIJqG",
        "outputId": "4fb621d4-0ba5-45ea-c702-1a9c99aa200e"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nInflation in Brazil during 2024 refers to the overall increase in prices of goods and services in the country. In November 2024, the monthly inflation rate was 0.5%, while the yearly inflation rate for 2024 was 6.2%. This means that the cost of living in Brazil increased by 6.2% compared to the previous year.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for forecast\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"weather\", \"inflation_rate\"],  # Include inflation_rate in input_variables\n",
        "    template=\"\"\"\n",
        "    Ми експортуємо апельсини з Бразилії. В 2021 експортували 200т, в 2022 - 190т, в 2023 - 210т, в 2024 який ще не закінчився - 220т.\n",
        "    Враховуючи погодні умови в Бразилії: {weather}, та світовий попит на апельсини з урахуванням економічної ситуації: {inflation_rate} ,\n",
        "    Зробіть прогноз, скільки тонн апельсинів ми зможемо експортувати у 2025 році.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Form the final prompt\n",
        "final_prompt = prompt_template.format(\n",
        "    weather=weather_info,\n",
        "    inflation_rate=inflation_rate,  # Use inflation_rate instead of inflation\n",
        ")\n",
        "\n",
        "# Response from the model with word limit\n",
        "response = llm(final_prompt, max_tokens=350)  # Assuming ~1.4 tokens per word\n",
        "\n",
        "# Result\n",
        "print(f\"Прогноз експорту на 2025 год:\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5B68PREJST6",
        "outputId": "3977075a-4945-40d9-9060-8ddbe810dc6f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Прогноз експорту на 2025 год:\n",
            "\n",
            "За умови, що погодні умови в Бразилії залишаться стабільними, а світовий попит на апельсини не зменшиться, можна припустити, що експорт апельсинів з Бразилії буде продовжуватися і у 2025 році. Однак, з урахуванням економічної ситуації в країні, можна очікувати зростання цін на апельсини, що може вплинути на обсяг експорту.\n",
            "\n",
            "З огляду на те, що в 2024 році було експортовано 220 тонн апельсинів, можна припустити, що у 2025 році цей обсяг може збільшитися на 5-10%. Таким чином, можна прогнозувати, що у 2025 році ми зможемо експортувати від 231 до 242 тонн апельсинів з Бразилії.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improvement suggestions:**\n",
        "\n",
        "1. Use Long-term Weather Forecasts: Implement other APIs to obtain long-term weather forecasts, helping to better predict future weather conditions in Brazil that might impact the orange harvest.\n",
        "2. Detailed Economic Demand Forecasts: Utilize more comprehensive APIs for economic demand forecasts instead of relying only on recent news. This would include analyzing detailed economic indicators that impact global demand for oranges.\n",
        "3. Analyze Global Production Trends: Incorporate data on global trends in fruit production to predict changes in demand, provided you have access to historical data.\n",
        "4. Machine Learning Model Training: Train a machine learning model using historical data on exports, weather, and demand. This model could provide more accurate forecasts for 2025.\n",
        "5. Add Visualization: Enhance the presentation of forecasts with visualizations using libraries like matplotlib or plotly. This will help in understanding the dynamics of demand and weather conditions through clear, interactive graphs.\n",
        "6. Seasonal Weather Patterns: Include seasonal weather pattern analysis to better predict weather-related impacts on orange production.\n",
        "7. Economic Indicators: Integrate other economic indicators such as GDP growth, consumer spending, and trade policies to improve demand forecasts.\n",
        "8. Supply Chain Analysis: Consider the effects of supply chain disruptions and logistics issues on orange exports.\n",
        "9. Competitor Analysis: Analyze export data from competing countries to understand their impact on global orange supply and demand.\n",
        "10. Consumer Trends: Monitor changes in consumer preferences and health trends that might affect orange consumption.\n",
        "11. Policy Changes: Stay updated on agricultural policies and trade agreements that might influence export regulations and tariffs.\n",
        "12. limate Change Impact: Model the long-term impacts of climate change on orange production in Brazil.\n",
        "13. Market Prices: Include analysis of current and projected market prices for oranges and related fruits.\n",
        "14. Weather Event Simulation: Simulate the impact of extreme weather events like droughts or floods on the orange harvest.\n",
        "15. Real-time Data Integration: Continuously integrate real-time data feeds to update forecasts dynamically.\n"
      ],
      "metadata": {
        "id": "80gSsTDyKGBK"
      }
    }
  ]
}